<!DOCTYPE html>
<html>
<head>
  <title>Research Computing Resources at Lehigh University</title>
  <meta charset="utf-8">
  <meta name="description" content="Research Computing Resources at Lehigh University">
  <meta name="author" content="https://researchcomputing.lehigh.edu">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/custom.css"></link>
<link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="assets/img/lu.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Research Computing Resources at Lehigh University</h1>
    <h2>Library &amp; Technology Services</h2>
    <p>https://researchcomputing.lehigh.edu<br/></p>
  </hgroup>
  <article></article>  
  <footer class = 'license'>
    <a href='http://creativecommons.org/licenses/by-sa/3.0/'>
    <img width = '80px' src = 'http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png'>
    </a>
  </footer>
</slide>
    

    <!-- SLIDES -->
    <slide class="lehigh" id="slide-1" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>About Us?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Who?</p>

<ul>
<li>Unit of Lehigh&#39;s Library &amp; Technology Services within the Center for Innovation in Teaching &amp; Learning</li>
</ul></li>
<li><p>Our Mission</p>

<ul>
<li>We enable Lehigh Faculty, Researchers and Scholars achieve their goals by providing various computational resources; hardware, software, and storage; consulting and training.</li>
</ul></li>
<li><p>Research Computing Staff</p>

<ul>
<li><strong>Alex Pacheco, Manager &#38; XSEDE Campus Champion</strong></li>
<li>Steve Anthony, HPC User Support &amp; System Administrator</li>
<li>Dan Brashler, Computing Consultant</li>
<li>Mary Jo Schulze, Software Specialist</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-2" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>What do we do?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Hardware Support

<ul>
<li>Provide system administration and support for Lehigh&#39;s HPC clusters.

<ul>
<li>6 University owned and 4 Faculty owned </li>
<li>4 University owned clusters to be decommissioned on Dec. 31, 2016.</li>
</ul></li>
<li>Assist with purchase, installation and administration of servers and clusters.</li>
</ul></li>
<li>Data Storage

<ul>
<li>Provide data management services including storing and sharing data. </li>
</ul></li>
<li>Software Support

<ul>
<li>Provide technical support for software applications, install software as requested and assist with purchase of software.</li>
</ul></li>
<li>Training &amp; Consulting

<ul>
<li>Provide education and training programs to facilitate use of HPC resources and general scientific computing needs.</li>
<li>Provide consultation and support for code development and visualization.</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-3" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Research Computing Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p><strong> Maia </strong></p>

<ul>
<li>Free 32-core Symmetric Multiprocessor (SMP) system available to all Lehigh Faculty, Staff and Students</li>
<li>dual 16-core AMD Opteron 6380 2.5GHz CPU</li>
<li>128GB RAM and 4TB HDD</li>
<li>Theoretical Performance: 640 GFLOPs (640 billion floating point operations per second)</li>
<li>Access: Batch Scheduled, no interactive access to Maia</li>
</ul>

<p>\[
GFLOPs = cores \times clock \times \frac{FLOPs}{cycle}
\]</p>

<p><a href="http://stackoverflow.com/questions/15655835/flops-per-cycle-for-sandy-bridge-and-haswell-sse2-avx-avx2">FLOPs for various AMD &amp; Intel CPU generation</a></p></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-4" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Research Computing Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><strong> Sol </strong>

<ul>
<li>Lehigh&#39;s Flagship High Performance Computing Cluster</li>
<li>8 nodes, dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 25MB Cache</li>
<li>Condo Investors

<ul>
<li>Dimitrios Vavylonis, Physics

<ul>
<li>1 node, dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 25MB Cache</li>
</ul></li>
<li>Wonpil Im, Biological Sciences

<ul>
<li>25 nodes, dual 12-core Intel Xeon E5-2670 v3 2.3Ghz CPU, 30 MB Cache</li>
</ul></li>
</ul></li>
<li>128 GB RAM and 1TB HDD per node</li>
<li>2:1 oversubscribed Infiniband EDR (100Gb/s) interconnect fabric</li>
<li>Theoretical Performance: 28.7 TFLOPs</li>
<li>Access: Batch Scheduled, interactive on login node for compiling, editing only</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-5" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Research Computing Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Resources due for retirement on December 31, 2016.</li>
<li>Available only to FY 2015-16 users</li>
<li><p><strong> Corona </strong></p>

<ul>
<li>40 nodes, dual 8-core AMD Opteron 6128 2GHz CPU, 32GB RAM and 1TB HDD</li>
<li>24 nodes, dual 8-core AMD Opteron 6128 2GHz CPU, 64GB RAM and 2TB HDD, 

<ul>
<li>Infiniband QDR (40Gb/s) interconnect fabric.</li>
</ul></li>
<li>Theoretical Performance: 8.2TFlops (8.2 trillion Flops)</li>
</ul></li>
<li><p><strong> Trit </strong>: Three SunFire x2270 Servers with  dual 4-core Intel Xeon X5570, 2.95GHz, 48GB RAM, 500GB HDD</p></li>
<li><p><strong> Capella </strong>: One node, quad 4-core AMD Opteron 8384, 2GHz, 64GB RAM, 2x 146GB HDD</p></li>
<li><p><strong> Cuda0 </strong>: One node, 6-core Intel Xeon X5650, 2.66GHz, 24GB RAM, 200GB HDD, 4 nVIDIA Fermi Devices (C2050, C2070, 2x M2070)</p></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-6" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>LTS Managed Faculty Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Ben Felzer, Earth &amp; Environmental Sciences

<ul>
<li>Eight nodes, dual 8-core Intel Xeon E5-2650v2, 2.6GHz, 64GB RAM

<ul>
<li>Theoretical Performance: 2.662TFlops</li>
</ul></li>
</ul></li>
<li>Heather Jaeger, Chemistry

<ul>
<li>Twenty nodes, dual 8-core Intel Xeon E5-2650v2, 2.6GHz, 64GB RAM

<ul>
<li>Theoretical Performance: 6.656TFlops</li>
</ul></li>
</ul></li>
<li>Jonas Baltrusaitis, Chemical Engineering

<ul>
<li>Three nodes, dual 16-core AMD Opteron 6376, 2.3Ghz, 128GB RAM

<ul>
<li>Theoretical Performance: 1.766TFlops</li>
</ul></li>
</ul></li>
<li>Keith Moored, Mechanical Engineering and Mechanics

<ul>
<li>Six nodes, dual 10-core Intel Xeon E5-2650v3, 2.3GHz, 64GB RAM, nVIDIA Tesla K80

<ul>
<li>Theoretical Performance: 4.416 TFlops (CPU) + 17.46TFlops (GPU)</li>
</ul></li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-7" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Apply for an account</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><a href="https://idmweb.cc.lehigh.edu/accounts/?page=hpc">Apply for an account at the LTS website</a>

<ul>
<li>Click on Services &gt; Account &amp; Password &gt; Lehigh Computing Account &gt; Request an account</li>
<li>Click on the big blue button &quot;Start Special Account Request&quot; &gt; Research Computing Account </li>
<li>Maia

<ul>
<li>Click on &quot;FREE Linux command-line computing&quot;</li>
</ul></li>
<li>Sol

<ul>
<li>Click on &quot;Fee-based research computing&quot;</li>
<li>Annual charge of $50/account paid by Lehigh Faculty or Research Staff, and</li>
<li>Annual charge for computing time</li>
</ul></li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-8" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Allocation Charges - Effective Oct. 1, 2016</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Cost per core-hour or service unit (SU) is 1&cent;</li>
<li>SU is defined as 1 hour of computing on 1 core of the Sol base compute node.

<ul>
<li>One base compute node of Sol consumes 20 SU/hour, 480 SU/day and 175,200 SU/year</li>
</ul></li>
<li><p>No free usage if allocation balance is zero</p></li>
<li><p>PIs can share allocations with their collaborators</p>

<ul>
<li>Minimum Annual Purchase of 50,000 SU - &#36;500/year</li>
<li>Additional Increments of 10,000 SU - &#36;100 per 10K increments</li>
<li>Fixed Allocation cycle: Oct 1 - Sep 30</li>
<li>Unused allocations do not rollover to next allocation cycle</li>
<li><em>Working on implementing a rolling allocation cycle, only for minimum purchase.</em></li>
<li>Total available computing time for purchase annually: 1.4M SUs or 1 year of continous computing on 8 nodes</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-9" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Example Allocation Request</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>PI requires 100K SUs of computing time per year
<ul class="unilist">
<li><span class="unibull">&#x2776;</span> One Purchase:
 <ul><li> 100K SU for &#36;1000/year</li></ul>
</li>
<li><span class="unibull">&#x2777;</span>Multiple Purchases:
 <ul><li> Initial 50K SUs for &#36;500/year.</li>
 <li> Multiple additional purchases of 10K SUs for &#36;100 each as required.</li></ul>
</li>
</ul></li>
<li><p>All 100K SUs (<span class="txtbull">&#x2776;</span> and <span class="txtbull">&#x2777;</span>) must be used up by Sep. 30 of next year.</p>

<ul>
<li>If rolling allocation cycle is implemented, then all 100K SUs (<span class="txtbull">&#x2776;</span> and <span class="txtbull">&#x2777;</span>) must be used up within 1 year of initial 50K purchase.</li>
</ul></li>
<li><p>Need more than 175K SU/year or</p></li>
<li><p>Do not think 1.4M SUs are enough for all HPC users, then</p></li>
<li><p>BECOME A CONDO INVESTOR</p></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-10" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Condo Investments</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>New sustainable model for High Performance Computing at Lehigh</li>
<li>Faculty (Condo Investor) purchase compute nodes from grants to increase overall capacity of Sol</li>
<li>LTS will provide for four years

<ul>
<li>System Administration, Power and Cooling, User Support for Condo Investments</li>
</ul></li>
<li>Condo Investor

<ul>
<li>receives annual allocation equivalent to their investment for four years</li>
<li>can utilize allocations on all available nodes, including nodes from other Condo Investors</li>
<li>allows idle cycles on investment to be used by other Sol users</li>
<li>unused allocation will not rollover to the next allocation cycle.</li>
<li>can purchase additional SUs in 10K increments (minimum 50K not required)

<ul>
<li> and must be consumed in current allocation cycle</li>
</ul></li>
</ul></li>
<li>Annual Allocation cycle is Oct. 1 - Sep. 30.</li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-11" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Condo Investors</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Two at initial launch </p>

<ul>
<li>Dimitrios Vavylonis, Physics

<ul>
<li>One dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 25MB Cache</li>
<li>Annual Allocation: 175,200 SUs</li>
</ul></li>
<li>Wonpil Im, Biological Sciences

<ul>
<li>25 dual 12-core Intel Xeon E5-2670 v3 2.3Ghz CPU, 30 MB Cache</li>
<li>Annual Allocation: 5,256,000 SUs</li>
</ul></li>
</ul></li>
<li><p>Total SU on Sol after Condo Investments: 6,832,800</p></li>
<li><p>Available capacity for additional investments: 38</p></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-12" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Accessing Research Computing Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>All Research Computing resources are accessible using ssh while on Lehigh&#39;s network</li>
<li>Sol: <code>ssh username@sol.cc.lehigh.edu</code></li>
<li>Maia: No direct access to Maia, instead login to the polaris</li>
<li>Polaris: <code>ssh username@polaris.cc.lehigh.edu</code>

<ul>
<li>Polaris is a gateway that also hosts the batch scheduler for Maia.</li>
<li>No computing software including compilers is available on Polaris.</li>
<li>Login to Polaris and request computing time on Maia including interactive access.</li>
</ul></li>
<li>If you are not on Lehigh&#39;s network, login to the ssh gateway to get to Research Computing resources.

<ul>
<li><code>ssh username@ssh.cc.lehigh.edu</code></li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-13" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Storage resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>LTS provides various storage options for research and teaching..</li>
<li>Some are cloud based and subject to Lehigh&#39;s Cloud Policy</li>
<li>For research, LTS provides a 1PB storage system called Ceph</li>
<li>Ceph is based on the Ceph software</li>
<li>Research groups can purchase a sharable project space on Ceph @ $200/TB/year</li>
<li>Ceph is in-house, built, operated and administered by LTS Research Computing Staff.

<ul>
<li>located in Data Center in EWFM with a backup cluster in Packard Lab</li>
</ul></li>
<li>HPC users can write job output directly to their Ceph volume</li>
<li>Ceph volume can be mounted as a network drive on Windows or CIFS on Mac and Linux

<ul>
<li><a href="http://lts.lehigh.edu/services/faq/ceph-faq">See Ceph FAQ</a> for more details</li>
</ul></li>
<li>Storage quota on

<ul>
<li>Maia: 5GB</li>
<li>Sol: 150GB</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-14" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Available Software</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Commercial and Free, Open source software is installed on

<ul>
<li>Maia: /zhome/Apps</li>
<li>Sol: /share/Apps</li>
</ul></li>
<li>Software is managed using module environment

<ul>
<li>Why? We may have different versions of same software or software built with different compilers</li>
<li>Module environment allows you to dynamically change your *nix environment based on software being used</li>
<li>Standard on many University and national High Performance Computing resource since circa 2011</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-15" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Software on Sol</h2>
  </hgroup>
  <article data-timings="">
    <p><img width = '960px' src = 'assets/img/sol-module.png'></p>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-16" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Module Command</h2>
  </hgroup>
  <article data-timings="">
    <table>
<tr><th width="300px">Command</th><th>Description</th></tr>
<tr><td><code>module avail</code></td><td> show list of software available on resource</td></tr>
<tr><td><code>module load abc</code></td><td> add software <code>abc</code> to your environment (modify your <code>PATH</code>, <code>LD_LIBRARY_PATH</code> etc as needed)</td></tr>
<tr><td><code>module unload abc</code></td><td> remove <code>abc</code> from your envionment</td></tr>
<tr><td><code>module swap abc1 abc2</code></td><td> swap <code>abc1</code> with <code>abc2</code> in your environment</td></tr>
<tr><td><code>module purge</code></td><td> remove all modules from your environment</td></tr>
<tr><td><code>module show abc</code></td><td> display what variables are added or modified in your environment</td></tr>
<tr><td><code>module help abc</code></td><td> display help message for the module <code>abc</code></td></tr>
</table>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-17" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Installed Software</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:45%;'>
  <ul>
<li>Chemistry/Materials Science

<ul>
<li>CPMD</li>
<li>GAMESS</li>
<li>Gaussian</li>
<li>NWCHEM</li>
<li>Quantum Espresso</li>
<li><em>VASP</em></li>
</ul></li>
<li>Molecular Dynamics

<ul>
<li><em>Desmond</em></li>
<li>GROMACS</li>
<li>LAMMPS</li>
<li>NAMD</li>
</ul></li>
</ul>

</div>
<div style='float:right;width:45%;'>
  <ul>
<li>Computational Fluid Dynamics

<ul>
<li><em>Abaqus</em></li>
<li>Ansys</li>
<li>Comsol</li>
<li>OpenFOAM</li>
<li>OpenSees</li>
</ul></li>
<li>Math

<ul>
<li>GNU Octave</li>
<li><em>Magma</em></li>
<li>Maple</li>
<li>Mathematica</li>
<li>Matlab</li>
</ul></li>
</ul>

</div>
  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-18" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>More Software</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:35%;'>
  <ul>
<li>Scripting Languages

<ul>
<li>R</li>
<li>Perl</li>
<li>Python</li>
</ul></li>
<li>Compilers

<ul>
<li>GNU</li>
<li>Intel</li>
<li>PGI</li>
</ul></li>
<li>Parallel Programming

<ul>
<li>MVAPICH2</li>
</ul></li>
</ul>

</div>
<div style='float:right;width:65%;'>
  <ul>
<li>Libraries

<ul>
<li>BLAS/LAPACK/GSL/SCALAPACK</li>
<li>Boost</li>
<li>FFTW</li>
<li>Intel MKL</li>
<li>HDF5</li>
<li>NetCDF</li>
<li>METIS/PARMETIS</li>
<li>PetSc</li>
<li>QHull/QRupdate</li>
<li>SuiteSparse</li>
<li>SuperLU</li>
</ul></li>
</ul>

</div>
  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-19" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>More Software</h2>
  </hgroup>
  <article data-timings="">
    
<div style='float:left;width:30%;'>
  <ul>
<li>Visualization Tools

<ul>
<li>Avogadro </li>
<li>GaussView</li>
<li>GNUPlot</li>
<li>VMD</li>
</ul></li>
<li>Other Tools

<ul>
<li>CMake</li>
<li><em>Gams</em></li>
<li><em>Gurobi</em> </li>
<li>Scons</li>
</ul></li>
</ul>

</div>
<div style='float:right;width:70%;'>
  <ul>
<li>You can always install a software in your home directory</li>
<li>Stay compliant with software licensing</li>
<li>Modify your .bashrc/.tcshrc to add software to your path, OR</li>
<li>create a module and dynamically load it so that it doesn&#39;t interfere 
with other software installed on the system

<ul>
<li>e.g. You might want to use openmpi instead of mvapich2 </li>
<li>the system admin may not want install it system wide for just one user</li>
</ul></li>
<li>Add the directory where you will install the module files to the variable 
MODULEPATH in .bashrc/.tcshrc</li>
</ul>

<pre><code class="sh"># My .bashrc file
export MODULEPATH=${MODULEPATH}:/home/alp514/modulefiles
</code></pre>

</div>
  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-20" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Module File Example</h2>
  </hgroup>
  <article data-timings="">
    <p><img width = '900px' src = 'assets/img/mcr.png'></p>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-21" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>XSEDE</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>The E<b>x</b>treme <b>S</b>cience and <b>E</b>ngineering <b>D</b>iscovery <b>E</b>nvironment (<strong>XSEDE</strong>) is the most advanced, powerful, and robust collection of integrated advanced digital resources and services in the world. </p></li>
<li><p>It is a single virtual system that scientists can use to interactively share computing resources, data, and expertise.</p></li>
<li><p>Scientists and engineers around the world use these resources and services—things like supercomputers, collections of data, and new tools—to make our lives healthier, safer, and better. </p></li>
<li><p>XSEDE, and the experts who lead the program, will make these resources easier to use and help more people use them.</p></li>
</ul>

<div style='float:left;width:60%;'>
  <ul>
<li><p>The five-year, $121-million project is supported by the National Science Foundation. </p></li>
<li><p>It replaces and expands on the NSF TeraGrid project.</p></li>
</ul>

</div>
<div style='float:right;width:40%;'>
  <p><img width = '400px' src = 'assets/img/xsede.png'></p>

</div>
  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-22" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>XSEDE Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>XSEDE is composed of multiple partner institutions known as Service Providers or SPs, each of which contributes one or more allocatable services. </p></li>
<li><p>Resources include High Performance Computing (HPC) machines, High Throughput Computing (HTC) machines, visualization, data storage, testbeds, and services. </p></li>
<li><p>Texas Advanced Computing Center (TACC) </p>

<ul>
<li>Stampede: 9.6 PFlops </li>
<li>Wrangler for Data Analytics</li>
<li>Maverick for Interactive Visualization and Data Analytics</li>
</ul></li>
<li><p>Louisiana State University</p>

<ul>
<li> SuperMIC: 925 TFlops </li>
</ul></li>
<li><p>National Institute for Computational Sciences (NICS) </p>

<ul>
<li>Darter: 248.9 TFlops</li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-23" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>XSEDE Resources</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>San Diego Supercomputing Center (SDSC) </p>

<ul>
<li>Comet: 2 PFlops</li>
<li>Gordon: 341 TFlops</li>
</ul></li>
<li><p>Indiana University </p>

<ul>
<li>Jetstream: a cloud-based, on-demand system for 24/7 access</li>
</ul></li>
<li><p>Pittsburgh Supercomputing Center</p>

<ul>
<li>Bridges: 1.3 PFlops (deployment in progress) </li>
</ul></li>
<li><p>Stanford University</p>

<ul>
<li>XStream: 1PFlops of 8 x NVIDIA Tesla K80 compute nodes</li>
</ul></li>
<li><p>Open Science Grid</p></li>
</ul>

<!--
<img width = '400px' src = 'assets/img/xsede.png'>
-->

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-24" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>How do I get started on XSEDE?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Apply for an account at the <a href="https://portal.xsede.org">XSEDE Portal</a>.</li>
<li>There is no charge to get an XSEDE portal account.<br></li>
<li>You need a portal account to register for XSEDE Tutorials and Workshops</li>
<li>To use XSEDE&#39;s compute and data resources, you need to have an allocation.</li>
<li>An allocation on a particular resource activates your account on that allocation.</li>
<li>Researchers and Educators from US universities and federal research labs can 
serve as Principle Investigators on XSEDE allocation.</li>
<li>A PI can add students to his/her allocations.</li>
<li>XSEDE also has a Campus Champion Program</li>
<li>A XSEDE Campus Champion is a local source of knowledge about high-performance 
and high-throughput computing and other digital services, opportunities and resources. </li>
<li>A Campus Champion can request start up allocations on all XSEDE resources to help 
local users with getting started on XSEDE resources.</li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-25" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Getting Help</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Issue with running jobs or need help to get started: 

<ul>
<li>Open a help ticket: <a href="http://go.lehigh.edu/rchelp">http://go.lehigh.edu/rchelp</a></li>
</ul></li>
<li>Investing in Sol

<ul>
<li>Contact Alex Pacheco or Steve Anthony</li>
</ul></li>
<li>More Information

<ul>
<li><a href="http://researchcomputing.lehigh.edu/services/condo">Condo Program and Available Equipment</a></li>
<li><a href="http://researchcomputing.lehigh.edu/services/proposalassist">Proposal Assistance</a></li>
<li><a href="http://libraryguides.lehigh.edu/researchdatamanagement">Data Management Plans</a></li>
<li><a href="https://researchcomputing.lehigh.edu">Research Computing</a></li>
<li><a href="https://researchcomputing.lehigh.edu/training">Research Computing Training</a></li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

<slide class="lehigh" id="slide-26" style="background:#F1E7C8;">
  <hgroup class=rcr>
    <h2>Upcoming Training</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>CITL Classroom (EWFM 379) on Tuesdays from 2PM - 4PM and Zoom Webcast.</p>

<ul>
<li>Using SLURM scheduler on Sol (Oct. 4)</li>
<li>Shell Scripting ( Oct. 11)</li>
<li>Research Data Management (Oct. 25)</li>
<li>Version Control with GIT (Nov. 1)</li>
<li>MATLAB (Nov. 8)</li>
<li>Enhancing Research Impact (Nov. 15)</li>
<li>Python Programming(Nov. 22)</li>
</ul></li>
<li><p>Subscribe</p>

<ul>
<li>Research Computing Mailing List: <a href="https://lists.lehigh.edu/mailman/listinfo/hpc-l">https://lists.lehigh.edu/mailman/listinfo/hpc-l</a></li>
<li>HPC Training Google Groups: <a href="mailto:hpctraining-list+subscribe@lehigh.edu">hpctraining-list+subscribe@lehigh.edu</a></li>
</ul></li>
</ul>

  </article>
  <footer class=rcr>
    <img src='assets/img/lulogo.jpg'></img>
  </footer>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='About Us?'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='What do we do?'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Research Computing Resources'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Research Computing Resources'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Research Computing Resources'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='LTS Managed Faculty Resources'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Apply for an account'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Allocation Charges - Effective Oct. 1, 2016'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Example Allocation Request'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Condo Investments'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Condo Investors'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Accessing Research Computing Resources'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Storage resources'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Available Software'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Software on Sol'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Module Command'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Installed Software'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='More Software'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='More Software'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='Module File Example'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='XSEDE'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='XSEDE Resources'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='XSEDE Resources'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='How do I get started on XSEDE?'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='Getting Help'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='Upcoming Training'>
         26
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>