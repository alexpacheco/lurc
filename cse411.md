---
title       : Lehigh University Research Computing
subtitle    : Library & Technology Services
author      : https://researchcomputing.lehigh.edu
job         : 
logo        : lu.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js      # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
--- .lehigh

## About Us?

* Who?
  - Unit of Lehigh's Library & Technology Services within the Center for Innovation in Teaching & Learning

* Our Mission
  - We enable Lehigh Faculty, Researchers and Scholars achieve their goals by providing various computational resources; hardware, software, and storage; consulting and training.
  
* Research Computing Staff
  - __Alex Pacheco, Manager & XSEDE Campus Champion__
  - Steve Anthony, HPC User Support & System Administrator
  - Dan Brashler, Computing Consultant
  - Mary Jo Schulze, Software Specialist

---.lehigh

## What do we do?

* Hardware Support
  - Provide system administration and support for Lehigh's HPC clusters.
     - 6 University owned and 4 Faculty owned 
     - 4 University owned clusters to be decommissioned on Dec. 31, 2016.
  - Assist with purchase, installation and administration of servers and clusters.
* Data Storage
  - Provide data management services including storing and sharing data. 
* Software Support
  - Provide technical support for software applications, install software as requested and assist with purchase of software.
* Training & Consulting
  - Provide education and training programs to facilitate use of HPC resources and general scientific computing needs.
  - Provide consultation and support for code development and visualization.

--- .lehigh

## Research Computing Resources

* <strong> Maia </strong>
  - Free 32-core Symmetric Multiprocessor (SMP) system available to all Lehigh Faculty, Staff and Students
  - dual 16-core AMD Opteron 6380 2.5GHz CPU
  - 128GB RAM and 4TB HDD
  - Theoretical Performance: 640 GFLOPs (640 billion floating point operations per second)
     * Each AMD Opteron 6380 (Piledriver) CPU is capable of 8 FLOPs 
  - Access: Batch Scheduled, no interactive access to Maia

  $$latex
  GFLOPs = cores \times clock \times \frac{FLOPs}{cycle}
  $$

  [FLOPs for various AMD & Intel CPU generation](http://stackoverflow.com/questions/15655835/flops-per-cycle-for-sandy-bridge-and-haswell-sse2-avx-avx2)

--- .lehigh

## Research Computing Resources

* <strong> Sol </strong> (Target production date October 1, 2016)
  - Lehigh's Flagship High Performance Computing Cluster
  - 8 nodes, dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 25MB Cache
     - Each Intel Xeon E5-26xx v3 (Haswell) CPU is capable of 16 FLOPs
  - Condo Investors
     - Dimitrios Vavylonis, Physics
          - 1 node, dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 25MB Cache
     - Wonpil Im, Biological Sciences
          - 25 nodes, dual 12-core Intel Xeon E5-2670 v3 2.5Ghz CPU, 30 MB Cache
  - 128 GB RAM and 1TB HDD per node
  - 2:1 oversubscribed Infiniband EDR (100Gb/s) interconnect fabric
  - Theoretical Performance: 28.7 TFLOPs
  - Access: Batch Scheduled, interactive on login node for compiling, editing only
  

--- .lehigh

## Research Computing Resources

* <strong> Corona </strong>
  - Formerly, Lehigh's Flagship High Performance Computing Cluster
  - 40 nodes, dual 8-core AMD Opteron 6128 2GHz CPU
      - 32GB RAM and 1TB HDD
  - 24 nodes, dual 8-core AMD Opteron 6128 2GHz CPU 
      - 64GB RAM and 2TB HDD, 
      - Infiniband QDR (40Gb/s) interconnect fabric.
  - Theoretical Performance: 8.2TFlops (8.2 trillion Flops)
     - Each AMD Opteron 6128 (AMD K10) CPU is capable of 4 FLOPs
  - Access: Batch scheduled, interactive on login node for compiling, editing only 
       - computer intensive tasks on the login node is strictly forbidden

--- .lehigh

## Research Computing Resources

* <strong> Trit </strong>
  - Three SunFire x2270 Servers with  dual 4-core Intel Xeon X5570, 2.95GHz, 48GB RAM, 500GB HDD
  - Theoretical Performance: 283.2GFlops

* <strong> Capella </strong>
  - One node, quad 4-core AMD Opteron 8384, 2GHz, 64GB RAM, 2x 146GB HDD
  - Theoretical Performance: 128GFlops

* <strong> Cuda0 </strong>
  - One node, 6-core Intel Xeon X5650, 2.66GHz, 24GB RAM, 200GB HDD
  - 4 nVIDIA Fermi Devices (C2050, C2070, 2x M2070)
  - Theoretical Performance: 63.840 GFlops (CPU) + 2.06 TFlops (GPU)

* Access: Interactive

--- .lehigh

## Faculty Owned Resources 

* Ben Felzer, Earth & Environmental Sciences
  - Eight nodes, dual 8-core Intel Xeon E5-2650v2, 2.6GHz, 64GB RAM
     * Theoretical Performance: 2.662TFlops
* Heather Jaeger, Chemistry
  - Twenty nodes, dual 8-core Intel Xeon E5-2650v2, 2.6GHz, 64GB RAM
     * Theoretical Performance: 6.656TFlops
* Jonas Baltrusaitis, Chemical Engineering
  - Three nodes, dual 16-core AMD Opteron 6376, 2.3Ghz, 128GB RAM
     * Theoretical Performance: 1.766TFlops
* Keith Moored, Mechanical Engineering and Mechanics
  - Six nodes, dual 10-core Intel Xeon E5-2650v3, 2.3GHz, 64GB RAM, nVIDIA Tesla K80
     * Theoretical Performance: 4.416 TFlops (CPU) + 17.46TFlops (GPU)

--- .lehigh

## Apply for an account

* [Apply for an account at the LTS website] (https://idmweb.cc.lehigh.edu/accounts/?page=hpc)
   - Click on Services > Account & Password > Lehigh Computing Account > Request an account
   - Click on the big blue button "Start Special Account Request" > Research Computing Account 
   - For Maia: click on "FREE Linux command-line computing"
   - For other resources: click on "Fee-based research computing"
* Fee-based resources carry an
   * Annual charge of \$50/account paid by Lehigh Faculty or Research Staff, and
   * Annual charge for computing time

--- .lehigh

## Allocation Charges

* New Policy: 1&cent; per core-hour or SU 
* Annual minimum purchase of 50,000 SUs and increments of 10,000 SUs.
   - paid for by your account sponsor
   - Total available computing time for purchase annually: 1,401,600 SUs or 1 year of continous computing on 8 nodes
* Condo Investors (Faculty who have increased Sol's capacity by purchasing 26 nodes).
   - Provided with annual computing time equivalent to investment at no charge.
   - DV purchased 1 20-core node is provided with 175,200 SUs annually.
   - WI purchase 25 24-core nodes is provided with 5,256,000 SUs annually.
   - Can purchase additional SUs in increments of 10,000 SUs if required.
* No 'free' computing time provided once allocation has been expended

--- .lehigh

## Accessing Research Computing Resources

* All Research Computing resources are accessible using ssh while on Lehigh's network
* `ssh username@cluster.cc.lehigh.edu`
   * where `cluster` is sol, corona, capella, cuda0 or trit{1,2,3}
* Maia: No direct access to Maia, instead login to the polaris
* Polaris: `ssh username@polaris.cc.lehigh.edu`
  - Polaris is a gateway that also hosts the batch scheduler for Maia.
  - No computing software including compilers is available on Polaris.
  - Login to Polaris to request computing time on Maia including interactive access to Maia.
* If you are not on Lehigh's network, login to the ssh gateway to get to Research Computing resources.
  - `ssh username@ssh.cc.lehigh.edu`

--- .lehigh

## What about Storage resources

* LTS provides various storage options.
* Some of these are in the cloud and subject to Lehigh's Cloud Policy
* For research, LTS provides a 1PB storage system called Ceph
* Ceph is based on the Ceph software
* Research groups can purchase a project space on Ceph @ $200/TB/year that can be shared
* Ceph is in-house, built, operated and administered by LTS Research Computing Staff.
  - located in Data Center in EWFM with a backup cluster in Packard Lab
* HPC users can write job output directly to their Ceph volume
* Ceph volume can be mounted as a network drive on Windows or CIFS on Mac and Linux
  - [See Ceph FAQ] (http://lts.lehigh.edu/services/faq/ceph-faq) for more details
* HPC User home directory quota
  - Maia: 5GB
  - Other Clusters: 150GB shared between Sol, Corona, Capella, Cuda and Trits

--- .lehigh

## Software available on HPC systems

* Most software is installed in /zhome/Apps.
  - may also be accessible from some workstations on campus
  - some software are installed locally. For e.g. CUDA compilers are local to Cuda0
* Software on Sol is compiled for modern cpus and is available at /share/Apps
* Software is managed using module environment
  - Why? We may have different versions of same software or software built with different compilers
  - Module environment allows you to dynamically change your *nix environment based on software being used
  - Standard on many University and national High Performance Computing resource since circa 2011

--- .lehigh

## Software on Sol

<img width = '960px' src = 'assets/img/sol-module.png'>

--- .lehigh

## How does module work?

* If you have access to HPC resources, try some of these commands
* `module avail`: show list of software available on resource
* `module load abc`: add software `abc` to your environment (modify your `PATH`, `LD_LIBRARY_PATH` etc as needed)
* `module unload abc`: remove `abc` from your envionment
* `module swap abc1 abc2`: swap `abc1` with `abc2` in your environment
* `module purge`: remove all modules from your environment
* `module show abc`: display what variables are added or modified in your environment
* `module help abc`: display help message for the module `abc`

--- .lehigh &twocol_width

## Installed Software

*** =left width:45%

* Chemistry/Materials Science
  - CPMD
  - GAMESS
  - Gaussian
  - NWCHEM
  - Quantum Espresso
  - *VASP*
* Molecular Dynamics
  - *Desmond*
  - GROMACS
  - LAMMPS
  - NAMD

*** =right width:45%

* Computational Fluid Dynamics
  - *Abaqus*
  - Ansys
  - Comsol
  - OpenFOAM
  - OpenSees
* Math
  - GNU Octave
  - *Magma*
  - Maple
  - Mathematica
  - Matlab

--- .lehigh  &twocol_width

## More Software

*** =left width:35%

* Scripting Languages
  - R
  - Perl
  - Python
* Compilers
  - GNU
  - Intel
  - PGI
* Parallel Programming
  - MVAPICH2

*** =right width:65%

* Libraries
  - BLAS/LAPACK/GSL/SCALAPACK
  - Boost
  - FFTW
  - Intel MKL
  - HDF5
  - NetCDF
  - METIS/PARMETIS
  - PetSc
  - QHull/QRupdate
  - SuiteSparse
  - SuperLU

--- .lehigh &twocol_width
 
## More Software

*** =left width:30%

* Visualization Tools
  - Avogadro 
  - GaussView
  - GNUPlot
  - VMD
* Other Tools
  - CMake
  - Gams
  - Gurobi 
  - Scons

*** =right width:70%

* You can always install a software in your home directory
* Stay compliant with software licensing
* Modify your .bashrc/.tcshrc to add software to your path, OR
* create a module and dynamically load it so that it doesn't interfere 
 with other software installed on the system
  - e.g. You might want to use openmpi instead of mvapich2 
  - the system admin may not want install it system wide for just one user
* Add the directory where you will install the module files to the variable 
  MODULEPATH in .bashrc/.tcshrc

```sh
# My .bashrc file
export MODULEPATH=${MODULEPATH}:/home/alp514/modulefiles
```

--- .lehigh

## Module File Example

<img width = '900px' src = 'assets/img/mcr.png'>

--- .lehigh

## Available Queues

* Maia

<table>
<tr><th>Queue Name</th><th>Max Walltime</th><th>Max Simultaneous Core-hours</th></tr>
<tr><td>smp-test</td><td>1 hour</td><td>4 core hours</td></tr>
<tr><td>smp</td><td>96 hours</td><td>384 core hours</td></tr>
</table>

<!--
* Corona

<table>
<tr><th>Queue Name</th><th>Max Walltime</th><th>Max Simultaneous Core-hours</th><th>Notes</th></tr>
<tr><td>p-ib</td><td>96/72</td><td>No Limit/288</td><td>Min 2 nodes/job, Max 24 nodes/user</td></tr>
<tr><td>normal</td><td>96/72</td><td>26880/7680</td><td>Max 34 nodes/user</td></tr>
<tr><td>short</td><td>24/12</td><td>384/192</td><td></td></tr>
<tr><td>bf</td><td>96</td><td></td><td>Max 65 nodes/user</td></tr>
</table>
-->

* Sol  (tentative)

<table>
<tr><th>Queue Name</th><th>Max Walltime</th><th>Max Simultaneous Core-hours</th><th>Notes</th></tr>
<tr><td>lts</td><td>72 hours</td><td></td><td>For use on 20-core nodes. Max 8 nodes/job</td></tr>
<tr><td>bio</td><td>48 hours</td><td></td><td>For use on 24-core nodes.</td></tr>
</table>

* Corona
  - Available queue are p, p-ib, normal and short.
  - No new accounts are being created on Corona.

--- .lehigh

## How to run jobs

* Write a submit script
* PBS for Maia/Corona and SLURM for Sol
  - need to have some background in shell scripting (bash/tcsh)
* Need to specify
   - Resources required (which depends on configuration)
       - number of nodes
       - number of processes per node
       - memory per node
   - How long do you want the resources
       - have an estimate for how long your job will run
   - Which queue to submit jobs

--- .lehigh &twocol_width

## Minimal submit script for Serial Jobs

*** =left width:48%


```bash
#!/bin/bash
#PBS -q smp
#PBS -l walltime=1:00:00
#PBS -l nodes=1:ppn=1
#PBS -l mem=4GB
#PBS -N myjob

cd ${PBS_O_WORKDIR}
./myjob < filename.in > filename.out

```

*** =right width:48%

```bash
#!/bin/bash
#SBATCH --partition=lts
#SBATCH --time=1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --job-name myjob

cd ${SLURM_SUBMIT_DIR}
./myjob < filename.in > filename.out

```

--- .lehigh

## Minimal submit script for MPI Jobs on Sol


```bash
#!/bin/bash
#SBATCH --partition=lts
#SBATCH --time=1:00:00
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=20
## For --partition=bio, use --ntasks-per-node=24
#SBATCH --job-name myjob

module load mvapich2

cd ${SLURM_SUBMIT_DIR}
srun ./myjob < filename.in > filename.out

exit
```

--- .lehigh

## Minimal submit script for OpenMP Jobs on Corona


```bash
#!/bin/tcsh
#SBATCH --partition=bio
# Directives can be combined on one line
#SBATCH --time=1:00:00 --nodes=1 --ntasks-per-node=24
#SBATCH --job-name myjob

cd ${SLURM_SUBMIT_DIR}
# Use either
setenv OMP_NUM_THREADS 24
./myjob < filename.in > filename.out

# OR
OMP_NUM_THREADS=16 ./myjob < filename.in > filename.out

exit
```


--- .lehigh .small

## Useful PBS Directives

<table class="pbs">
<tr>
<td><code>#PBS -q queuename</code></td><td> Submit job to the <em>queuename</em> queue.</td>
</tr>
<tr>
<td><code>#PBS -l walltime=hh:mm:ss</code></td><td> Request resources to run job for <em>hh</em> hours, <em>mm</em> minutes and <em>ss</em> seconds.</td>
</tr>
<tr>
<td><code>#PBS -l nodes=m:ppn=n</code></td><td> Request resources to run job on <em>n</em> processors each on <em>m</em> nodes.</td>
</tr>
<tr>
<td><code>#PBS -l mem=xGB</code></td><td> Request <em>xGB</em> per node requested, applicable on Maia only</td>
</tr>
<tr>
<td><code>#PBS -N jobname</code></td><td> Provide a name, <em>jobname</em> to your job.</td>
</tr>
<tr>
<td><code>#PBS -o filename.out</code></td><td> Write PBS standard output to file filename.out.</td>
</tr>
<tr>
<td><code>#PBS -e filename.err</code></td><td> Write PBS standard error to file filename.err.</td>
</tr>
<tr>
<td><code>#PBS -j oe</code></td><td> Combine PBS standard output and error to the same file.</td>
</tr>
<tr>
<td><code>#PBS -m status</code></td><td> Send an email after job status status is reached. <br />
 status can be a (abort), b (begin) or e (end) <br />
 The arguments can be combined, for e.g. abe will send email when job begins and either aborts or ends</td>
</tr>
<td><code>#PBS -M your email address</code></td><td> Address to send email.</td>
</tr>
</table>

--- .lehigh

## Useful PBS/SLURM environmental variables

<table class="pbs">
<tr>
 <td><code> PBS_O_WORKDIR</code></td><td> Directory where the <code>qsub</code> command was executed</td><td><code>SLURM_SUBMIT_DIR</code></td>
</tr>
<tr>
 <td><code> PBS_NODEFILE</code></td><td> Name of the file that contains a list of the HOSTS provided for the job</td><td><code>SLURM_JOB_NODELIST</code></td>
</tr>
<tr>
 <td><code> PBS_NP</code></td><td> Total number of cores for job</td><td><code>SLURM_NTASKS</code></td>
</tr>
<tr>
 <td><code> PBS_JOBID</code></td><td> Job ID number given to this job</td><td><code>SLURM_JOBID</code></td>
</tr>
<tr>
 <td><code> PBS_QUEUE</code></td><td> Queue job is running in</td><td><code></code></td>
</tr>
<tr>
 <td><code> PBS_WALLTIME</code></td><td> Walltime in secs requested</td><td><code></code></td>
</tr>
<tr>
 <td><code> PBS_JOBNAME</code></td><td> Name of the job. This can be set using the -N option in the PBS script</td><td><code></code></td>
</tr>
<tr>
 <td><code> PBS_ENVIRONMENT</code></td><td> Indicates job type, PBS_BATCH or PBS_INTERACTIVE</td><td><code></code></td>
</tr>
<tr>
 <td><code> PBS_O_SHELL</code></td><td>	value of the SHELL variable in the environment in which qsub was executed</td><td><code></code></td>
</tr>
<tr>
 <td><code> PBS_O_HOME</code></td><td> Home directory of the user running qsub</td><td><code></code></td>
</tr>
</table>

--- .lehigh

## Submitting & Monitoring Jobs

<table>
<tr><th>PBS Command</th><th>Description</th><th>SLURM Command</th></tr>
<tr>
  <td><code>qsub filename</code></td><td>Submit <em>filename</em> to job scheduler</td><td><code>sbatch filename</code></td>
</tr>
<tr>
  <td><code>qstat</code> </td><td>check job status (all jobs)</td><td><code>squeue</code></td>
</tr>
<tr>
  <td><code>qstat -u username</code></td><td>check job status of user <em>username</em></td><td><code>squeue -u username</code></td>
</tr>
<tr>
  <td><code>qstat -a</code></td><td>More information than that given by <em>qstat</em></td><td><code>squeue -l</code></td>
</tr>
<tr>
  <td><code>qdel jobid</code></td><td>Cancel your job identified by <em>jobid</em></td><td><code>scancel jobid</code></td>
</tr>
<tr>
  <td><code>showstart jobid</code></td><td>Show <strong>estimated</strong> start time of job identified by <em>jobid</em></td><td><code>squeue --start</code></td>
</tr>
<tr>
  <td><code>checkjob jobid</code></td><td>Check status of your job identified by <em>jobid</em></td><td><code>scontrol show job jobid</code></td>
</tr>
<tr>
  <td><code>qhold jobid</code></td><td>Put your job identified by <em>jobid</em> on hold</td><td><code>scontrol hold jobid</code></td>
</tr>
<tr>
  <td><code>qrls jobid</code></td><td>Release the hold that <strong>you put</strong> on <em>jobid</em></td><td><code>scontrol release jobid</code></td>
</tr>
</table>


--- .lehigh

## Training

* CITL Classroom (EWFM 379) on Tuesdays from 2PM - 4PM and Zoom Webcast.
  - A Brief Introduction to Linux (Sep. 6)
  - Linux: Basic Commands & Environment (Sep. 13)
  - Storage Options at Lehigh (Sep. 20)
  - Research Computing at Lehigh (Sep. 27)
  - Using SLURM scheduler on Sol (Oct. 4)
  - Shell Scripting ( Oct. 11)
  - Research Data Management (Oct. 25)
  - Version Control with GIT (Nov. 1)
  - MATLAB (Nov. 8)
  - Enhancing Research Impact (Nov. 15)
  - Python Programming(Nov. 22)
 

--- .lehigh

## Full Day Workshops

* During the summer we provide full day workshops on programming topics
* Summer 2015 Workshops
  - Modern Fortran Programming
  - C Programming
* We also host full day workshops broadcast from other Supercomputing Centers
  - XSEDE HPC Monthly Workshop: OpenACC (Dec. 2014)
  - XSEDE HPC Summer BootCamp: OpenMP, OpenACC, MPI and Hybrid Programming (Jun. 2015 &amp; 2016)
  - XSEDE HPC Monthly Workshop: Big Data (Nov. 2015)


--- .lehigh &twocol_width

## XSEDE

* The E<b>x</b>treme <b>S</b>cience and <b>E</b>ngineering <b>D</b>iscovery <b>E</b>nvironment (<strong>XSEDE</strong>) is the most advanced, powerful, and robust collection of integrated advanced digital resources and services in the world. 

* It is a single virtual system that scientists can use to interactively share computing resources, data, and expertise.

* Scientists and engineers around the world use these resources and services—things like supercomputers, collections of data, and new tools—to make our lives healthier, safer, and better. 

* XSEDE, and the experts who lead the program, will make these resources easier to use and help more people use them.


*** =left width:60%
* The five-year, $121-million project is supported by the National Science Foundation. 

* It replaces and expands on the NSF TeraGrid project.

*** =right width:40%

<img width = '400px' src = 'assets/img/xsede.png'>

--- .lehigh 

## XSEDE Resources

* XSEDE is composed of multiple partner institutions known as Service Providers or SPs, each of which contributes one or more allocatable services. 

* Resources include High Performance Computing (HPC) machines, High Throughput Computing (HTC) machines, visualization, data storage, testbeds, and services. 


* Texas Advanced Computing Center (TACC) 
   - Stampede: 9.6 PFlops (8<sup>th</sup> fastest supercomputer in the world)
   - Wrangler for Data Analytics
   - Maverick for Interactive Visualization and Data Analytics
* Louisiana State University
   -  SuperMIC: 925 TFlops (116<sup>th</sup> fastest)
* National Institute for Computational Sciences (NICS) 
   - Darter: 248.9 TFlops

--- .lehigh 

## XSEDE Resources

* San Diego Supercomputing Center (SDSC) 
   - Comet: 2 PFlops
   - Gordon: 341 TFlops

* Indiana University 
   - Jetstream

* Pittsburgh Supercomputing Center
   - Bridges 

* Stanford University
   - XStream 

* Open Science Grid

<!--
<img width = '400px' src = 'assets/img/xsede.png'>
-->

--- .lehigh 

## How do I get started on XSEDE?

* Apply for an account at the [XSEDE Portal](https://portal.xsede.org).
* There is no charge to get an XSEDE portal account.  
* You need a portal account to register for XSEDE Tutorials and Workshops
* To use XSEDE's compute and data resources, you need to have an allocation.
* An allocation on a particular resource activates your account on that allocation.
* Researchers and Educators from US universities and federal research labs can 
serve as Principle Investigators on XSEDE allocation.
* A PI can add students to his/her allocations.
* XSEDE also has a Campus Champion Program
* A XSEDE Campus Champion is a local source of knowledge about high-performance 
and high-throughput computing and other digital services, opportunities and resources. 
* A Campus Champion can request start up allocations on all XSEDE resources to help 
 local users with getting started on XSEDE resources.

--- .lehigh

## Contact Us

* Issue with running jobs or need help to get started: 
  * Open a help ticket: <http://go.lehigh.edu/rchelp>
* My contact info
  * eMail:  <alp514@lehigh.edu>
  * Tel: (610) 758-6735 
  * Location: Room 296, EWFM Computing Center
  * [My Schedule] (https://www.google.com/calendar/embed?src=alp514%40lehigh.edu&ctz=America/New_York )
* More Information
  * [Research Computing] (https://researchcomputing.lehigh.edu)
  * [Research Computing Training] (https://researchcomputing.lehigh.edu/training)
* Subscribe
     * Research Computing Mailing List: <https://lists.lehigh.edu/mailman/listinfo/hpc-l>
     * HPC Training Google Groups: <mailto:hpctraining-list+subscribe@lehigh.edu>


 
