---
title       : Introduction to Linux & HPC
subtitle    : Research Computing, Library & Technology Services
author      : https://researchcomputing.lehigh.edu
job         : 
logo        : lu.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js      # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
--- .class

## About Us?

* Who?
  - Unit of Lehigh&#39;s Library & Technology Services within the Center for Innovation in Teaching & Learning

* Our Mission
  - We enable Lehigh Faculty, Researchers and Scholars achieve their goals by providing various computational resources; hardware, software, and storage; consulting and training.
  
* Research Computing Staff
  - __Alex Pacheco, Manager & XSEDE Campus Champion__
  - Steve Anthony, System Administrator
  - Dan Brashler, CAS Senior Computing Consultant
  - Sachin Joshi, Data Analyst & Visualization Specialist

--- .class

## What do we do?

* Hardware Support
  - Provide system administration and support for Lehigh&#39;s HPC clusters.
     - 1 University owned and 3 Faculty owned 
  - Assist with purchase, installation and administration of servers and clusters.
* Data Storage
  - Provide data management services including storing and sharing data. 
* Software Support
  - Provide technical support for software applications, install software as requested and assist with purchase of software.
* Training & Consulting
  - Provide education and training programs to facilitate use of HPC resources and general scientific computing needs.
     * https://go.lehigh.edu/hpcseminars
  - Provide consultation and support for code development and visualization.


--- .class #id

## Research Computing Resources

* <strong> [Sol](https://go.lehigh.edu/sol) </strong>
  - Lehigh&#39;s Flagship High Performance Computing Cluster
  - 9 nodes, dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 25MB Cache, 128GB RAM
  - 33 nodes, dual 12-core Intel Xeon E5-2670 v3 2.3Ghz CPU, 30 MB Cache, 128GB RAM
  - 14 nodes, dual 12-core Intel Xeon E5-2650 v4 2.3Ghz CPU, 30 MB Cache, 64GB RAM
  - 1 node, dual 8-core Intel Xeon 2630 v3 2.4GHz CPU, 20 MB Cache, 512GB RAM
  - 24 nodes, dual 18-core Intel Xeon Gold 6140 2.3GHz CPU, 24.7 MB Cache, 192GB RAM
  - 6 nodes, dual 18-core Intel Xeon Gold 6240 2.6GHz, 24.75 MB Cache, 192GB RAM
  - 72 nVIDIA GTX 1080 & 48 nVIDIA RTX 2080TI GPU cards
  - 1TB HDD per node
  - 2:1 oversubscribed Infiniband EDR (100Gb/s) interconnect fabric
  - Theoretical Performance: 93.184 TFLOPs (CPU) + 36.130 TFLOPs (GPU)

--- .class #id

## LTS Managed Faculty Resources 

* __Monocacy__: Ben Felzer, Earth & Environmental Sciences
  - Eight nodes, dual 8-core Intel Xeon E5-2650v2, 2.6GHz, 64GB RAM
     * Theoretical Performance: 2.662TFlops
* __Baltrusaitislab__: Jonas Baltrusaitis, Chemical Engineering
  - Three nodes, dual 16-core AMD Opteron 6376, 2.3Ghz, 128GB RAM
     * Theoretical Performance: 1.766TFlops
* __Pisces__: Keith Moored, Mechanical Engineering and Mechanics
  - Six nodes, dual 10-core Intel Xeon E5-2650v3, 2.3GHz, 64GB RAM, nVIDIA Tesla K80
     * Theoretical Performance: 4.416 TFlops (CPU) + 17.46TFlops (GPU)
* Pavo : decommissioned faculty cluster for prototyping future resources
  - Twenty nodes, dual 8-core Intel Xeon E5-2650v2, 2.6GHz, 64GB RAM
     * Theoretical Performance: 6.656TFlops
     * 3 nodes available for 1 hour debug runs. 

--- .class

## Total Computational Resources Supported

```{r hpc_cluster,echo=F, warning=F}
cluster <- tribble(
    ~"Cluster",~"Cores",~"CPU Memory",~"CPU TFLOPs",~"GPUs",~"CUDA Cores",~"GPU Memory",~"GPU TFLOPS",
#    "Maia",32,128,0.640,0,0,0,0,
    "Monocacy",128,512,2.662,0,0,0,0,
    "Pavo",320,1280,6.656,0,0,0,0,
    "Baltrusaitis",96,384,1.766,0,0,0,0,
    "Pisces",120,384,3.840,12,29952,144,17.422,
    "Sol",2404,12544,93.184,120,393216,1104,36.130)
cluster %>% summarise_if(is.numeric, sum) %>% 
    mutate(Cluster="Total") %>% 
    select("Cluster","Cores":"GPU TFLOPS") -> total
rbind(cluster,total) %>% kable()
```

- Monocacy, Baltrusaitis and Pisces: decommissioning scheduled for Sep 30, 2021.

--- .class #id

## Accessing Research Computing Resources

* Sol: accessible using ssh while on Lehigh&#39;s network
    ```{bash eval=F}
    ssh username@sol.cc.lehigh.edu
    ```
   * Windows PC require a SSH client such as [MobaXterm](https://mobaxterm.mobatek.net/) or [Putty](https://putty.org/).
   * Mac and Linux PC&#39;s, ssh is built in to the terminal application. 
* If you are not on Lehigh&#39;s network, login to the ssh gateway
   ```{bash eval=F}
    ssh username@ssh.cc.lehigh.edu
   ``` 
   and then login to sol as above
  -  Alternatively,
  ```{bash eval=F}
  ssh -J username@ssh.cc.lehigh.edu username@sol.cc.lehigh.edu
  ```
  - [Click here](https://confluence.cc.lehigh.edu/x/JhH5Bg) to learn how to configure MobaXterm to use the SSH Gateway.

--- .class 

## Open Ondemand

- Open, Interactive HPC via the Web 
    - Easy to use, plugin-free, web-based access to supercomputers 
    - File Management 
    - Command-line shell access 
    - Job management and monitoring 
    - Various Applications 
- NSF-funded project 
    - SI2-SSE-1534949 and CSSI-Software-Frameworks-1835725 
    - Ohio Supercomputing Center 
    - Deployed at dozens of sites (universities, supercomputing centers)
- At Lehigh: https://hpcportal.cc.lehigh.edu
    - Lehigh IP or VPN required

--- .class

## What about Storage resources

* LTS provides various storage options for research and teaching.
* Some are cloud based and subject to Lehigh&#39;s Cloud Policy.
* For research, LTS provides a 768TB storage system called [Ceph](https://go.lehigh.edu/ceph).
* Ceph is based on the Ceph software.
* Research groups can purchase a sharable project space on Ceph @ &#36;375/TB with a 5 year duration.
* Ceph is in-house, built, operated and administered by LTS Research Computing Staff.
  - located in Data Center in EWFM with a backup cluster in Packard Lab
* HPC users can write job output directly to their Ceph volume
* Ceph volume can be mounted as a network drive on Windows or CIFS on Mac and Linux
  - [See Ceph FAQ] (http://lts.lehigh.edu/services/faq/ceph-faq) for more details
* Annual HPC User account fees waived for PIs who purchase a 1TB Ceph space for life of Ceph i.e. 5 years 

--- .class #id

## Network Layout Sol &amp; Ceph Storage Cluster

<img class="fullwidth" src='assets/img/hpcnetwork.png'>

--- .class

## How do I get started using HPC resources for this course

* Login to sol using the SSH Client or the web portal

* Linux is the Operating System installed on all HPC resources.
   - You should see something like `[alp514@sol ~]$ ` if you are logged into sol
   - This is known as the command prompt
   - sol is the head/login for the cluster. 
   - __Do not run computation on this node.__
   - Editing files, compiling code are acceptable computation on this node.
   - Running intense computation on this node causes a high load on the storage
     that would cause other users jobs to run slow.


--- .class

## What is Linux?

* Linux is an operating system that evolved from a kernel created by Linus Torvalds when he was a student at the University of Helsinki.
* It’s meant to be used as an alternative to other operating systems, Windows, Mac OS, MS-DOS, Solaris and others.
* Linux is the most popular OS used in a Supercomputer

| OS Family | Count | Share % |
|:---------:| -----:| -------:|
| Linux | 498 | 99.6 |
| Unix  |   2 |   .4 |

* If you are using a Supercomputer/High Performance Computer for your research, it will be based on a *nix OS.
* It is required/neccessary/mandatory to learn Linux Programming (commands, shell scripting) if your research involves use of High Performance Computing or Supercomputing resources.

--- .class

## Where is Linux used?

* Linux distributions are tailored to different requirements such as
  - Server
  - Desktop
  - Workstation
  - Routers
  - Embedded devices
  - Mobile devices (Android is a Linux-based OS)
* Almost any software that you use on windows has a roughly equivalent software on Linux, most often multiple equivalent software
  - e.g. Microsoft Office equivalents are OpenOffice.org, LibreOffice, KOffice
  - [ Visit for complete list ]
    (http://wiki.linuxquestions.org/wiki/Linux_software_equivalent_to_Windows_software)
* Linux offers you freedom, to choose your desktop environment, software.

--- .class .tiny

## What is a Linux OS, Distro, Desktop Environment?

* Many software vendors release their own packaged Linux OS (kernel, applications) known as distribution
* Linux distribution = Linux kernel + GNU system utilities and libraries + Installation scripts + Management utilities etc.
    - Debian, Ubuntu, Mint
    - Red Hat, Fedora, __CentOS__
    - Slackware, openSUSE, SLES, SLED
    - Gentoo
* Application packages on Linux can be installed from source or from customized packages
    - deb: Debian based distros e.g. Debian, Ubuntu, Mint
    - rpm: Red Hat based distros, Slackware based distros.
* Linux distributions offer a variety of desktop environment.
    - K Desktop Environment (KDE)
    - GNOME
    - __XFCE__
    - Lightweight X11 Desktop Environment (LXDE)
    - Cinnamon
    - __MATE__
    - Dynamic Window Manager

--- .class .small

## Difference between Shell and Command

* What is a Shell?
    - The command line interface is the primary interface to Linux/Unix operating
systems.
    - Shells are how command-line interfaces are implemented in Linux/Unix.
    - Each shell has varying capabilities and features and the user should choose the shell that best suits their needs.
    - The shell is simply an application running on top of the kernel and provides a powerful interface to the system.
* What is a command and how do you use it?
    - __command__ is a directive to a computer program acting as an interpreter of some kind,
in order to perform a specific task.
    - __command prompt__ (or just __prompt__) is a sequence of (one or more) characters used in a command-line interface to indicate readiness to accept commands.
    - Its intent is to literally prompt the user to take action.
    - A prompt usually ends with one of the characters $, %, #, :, > and often includes other information, such as the path of the current working directory.

--- .class .small

## Types of Shell

* sh : Bourne Shell
  - Developed by Stephen Bourne at AT&T Bell Labs
* csh : C Shell
  - Developed by Bill Joy at University of California, Berkeley
* ksh : Korn Shell
  - Developed by David Korn at AT&T Bell Labs
  - backward-compatible with the Bourne shell and includes many features of the C shell
* bash : Bourne Again Shell
  - Developed by Brian Fox for the GNU Project as a free software replacement for the Bourne shell (sh).
  - Default Shell on Linux and Mac OSX
* tcsh : TENEX C Shell
  - Developed by Ken Greer at Carnegie Mellon University
  - It is essentially the C shell with programmable command line completion, command-line editing, and a few other features.


--- .class

## Directory Structure

* All files are arranged in a hierarchial structure, like an inverted tree.
*  The top of the hierarchy is traditionally called __root__ (written as a slash / )

<img width = '900px' src = 'assets/img/LinuxFS.png'>

--- .class

## Relative & Absolute Path
* Path means a position in the directory tree.
* You can use either the relative path or absolute path
* In relative path expression
  -  . (one dot or period) is the current working directory
  -  .. (two dots or periods) is one directory up
  -  You can combine . and .. to navigate the filee system hierarchy.
  -  the path is not defined uniquely and does depend on the current path.
  -  ../../tmp is unique only if your current working directory is your home directory.
* In absolute path expression
  -  the path is defined uniquely and does not depend on the current path
  -  /tmp is unique since /tmp is the abolute path


--- .class

## Some Linux Terms (also called variables)

* __HOME__ : Your Home Directory on the system, /home/username
  - This is where you should be when you login the first time
  - Don&#39;t believe me, type `pwd` and hit enter
* __PATH__ : List of directories to search when executing a command
  - Enter `avogadro` at the command prompt
  - You should see an error saying `command not found`
* __LD_LIBRARY_PATH__ : List of directories to look for libraries executing code



--- .class .small

## Linux Commands

* `man` shows the manual for a command or program.
  - `man pwd`
* `pwd`: print working directory, gives the absolute path of your current location in the directory hierarchy
* `echo`: prints whatever follows to the screen
  - `echo $HOME`: prints the contents of the variable __HOME__ i.e. your home directory to the screen
* `cd dirname`: change to folder called `dirname`
  - default `dirname` is your home directory
  - Useful option 
       - `cd -` go to previous directory
* `mkdir dirname`: create a directory called `dirname`
  - Create a directory, any name you want and cd to that directory 
       - `mkdir me450` followed by `cd me450`
  - Useful option
       - `mkdir -p dir1/dir2/dir3` create intermediate directories if they do not exist
  

--- .class .tiny

## Linux Command (contd)

* `cp file1 file2`: command to copy file1 to file2
  - Useful options:
      - `cp -r /home/alp514/Workshop/sum2017 .` copy directories recursively
      - `cp -p ~alp514/in.lj ${HOME}/` preserve time stamps
* `rm file1`: delete a file called file1 
  - Useful options
      - `rm -i` to be prompted for confirmation
      - `rm -r` to delete directories recursively 
      - `rm -f` delete without prompting - VERY DANGEROUS
* `ls dirname`: list contents of _dirname_ (leave blank for current directory)
  - Useful options
      - `ls -l` show long form listing
      - `ls -a` show hidden files
      - `ls -t` sort by timestamp newest first, best when combined with `-l`
      - `ls -r` reverse sort (default alphabetical), best when combined with `-l -t` or `-lt`
* `alias`: create a shortcut to another command or name to execute a long string.
  - `alias rm="/bin/rm -i"`

---  .class

## File Editing

* The two most commonly used editors on Linux/Unix systems are:
  -  vi or vim (vi improved)
  -  emacs
* vi/vim is installed by default on Linux/Unix systems and has only a command line interface (CLI).
* emacs has both a CLI and a graphical user interface (GUI).
* Other editors that you may come across on *nix systems
  - kate: default editor for KDE.
  - gedit: default text editor for GNOME desktop environment.
  - gvim: GUI version of vim
  - pico: console based plain text editor
  - nano: GNU.org clone of pico
  - kwrite: editor by KDE.


--- .class  &twocol_width

## vi commands

*** =left width:45%

<table>
<tr><th>Inserting/Appending Text</th><th>Command</th></tr>
<tr>
<td>insert at cursor</td><td>i</td>
</tr><tr>
<td>insert at beginning of line</td><td>I</td>
</tr><tr>
<td>append after cursor</td><td>a</td>
</tr><tr>
<td>append at end of line</td><td>A</td>
</tr><tr>
<td>newline after cursor in insert mode</td><td>o</td>
</tr><tr>
<td>newline before cursor in insert mode</td><td>O</td>
</tr><tr>
<td>append at end of line</td><td>ea</td>
</tr><tr>
<td>exit insert mode</td><td>ESC</td>
</tr>
</table>

*** =right width:45%

<table>
<tr><th>Cursor Movement</th><th>Command</th></tr>
<tr><td>move left</td><td>h</td></tr>
<tr><td>move down</td><td>j</td></tr>
<tr><td>move up</td><td>k</td></tr>
<tr><td>move right</td><td>l</td></tr>
<tr><td>jump to beginning of line</td><td>^</td></tr>
<tr><td>jump to end of line</td><td>$</td></tr>
<tr><td>goto line n</td><td>nG</td></tr>
<tr><td>goto top of file</td><td>1G</td></tr>
<tr><td>goto end of file</td><td>G</td></tr>
<tr><td>move one page up</td><td>CNTRL-u</td></tr>
<tr><td>move one page down</td><td>CNTRL-d</td></tr>
</table>


--- .class  &twocol_width

## vi commands

*** =left width:45%

<table>
<tr><th>File Manipulation</th><th>Command</th></tr>
<tr><td>save file</td><td>:w</td></tr>
<tr><td>save file and exit</td><td>:wq</td></tr>
<tr><td>quit</td><td>:q</td></tr>
<tr><td>quit without saving</td><td>:q!</td></tr>
<tr><td>delete a line</td><td>dd</td></tr>
<tr><td>delete n lines</td><td>ndd</td></tr>
<tr><td>paste deleted line after cursor</td><td>p</td></tr>
<tr><td>paste before cursor</td><td>P</td></tr>
<tr><td>undo edit</td><td>u</td></tr>
<tr><td>delete from cursor to end of line</td><td>D</td></tr>
</table>

*** =right width:45%

<table>
<tr><th>File Manipulation</th><th>Command</th></tr>
<tr><td>replace a character</td><td>r</td></tr>
<tr><td>join next line to current</td><td>J</td></tr>
<tr><td>change a line</td><td>cc</td></tr>
<tr><td>change a word</td><td>cw</td></tr>
<tr><td>change to end of line</td><td>c$</td></tr>
<tr><td>delete a character</td><td>x</td></tr>
<tr><td>delete a word</td><td>dw</td></tr>
<tr><td>edit/open file </td><td>:e file</td></tr>
<tr><td>insert file </td><td>:r file</td></tr>
</table>


--- .class #id

## Available Software

* A variety of commercial, free and open source software is available
  - https://go.lehigh.edu/hpcsoftware
* Software is managed using module environment
  - Why? We may have different versions of same software or software built with different compilers
  - Module environment allows you to dynamically change your *nix environment based on software being used
  - Standard on many University and national High Performance Computing resource since circa 2011
* LTS provides [licensed and open source software](https://software.lehigh.edu) for Windows, Mac and Linux and [Gogs](https://gogs.cc.lehigh.eu), a self hosted Git Service or Github clone 


--- .class #id

## Module Command

| Command | Description |
|:-------:|:-----------:|
| <code>module avail</code> | show list of software available on resource |
| <code>module load abc</code> | add software <code>abc</code> to your environment (modify your <code>PATH</code>, <code>LD_LIBRARY_PATH</code> etc as needed) |
| <code>module unload abc</code> | remove <code>abc</code> from your environment |
| <code>module swap abc1 abc2</code> | swap <code>abc1</code> with <code>abc2</code> in your environment |
| <code>module purge</code> | remove all modules from your environment |
| <code>module show abc</code> | display what variables are added or modified in your environment |
| <code>module help abc</code> | display help message for the module <code>abc</code> |

* Users who prefer not to use the module environment will need to modify their
  .bashrc or .tcshrc files. Run `module show` for list variables that need
  modified, appended or prepended

--- .class &twocol_width

## Installed Software

*** =left width:45%

* Chemistry/Materials Science
  - **CPMD**
  - **GAMESS**
  - Gaussian
  - **NWCHEM**
  - **Quantum Espresso**
  - **VASP** (Restricted Access)
* Molecular Dynamics
  - **Desmond**
  - **GROMACS**
  - **LAMMPS**
  - **NAMD**

<span class="tiny strong">__MPI enabled__</span>

*** =right width:45%

* Computational Fluid Dynamics
  - Abaqus
  - Ansys
  - Comsol
  - **OpenFOAM**
  - OpenSees
* Math
  - GNU Octave
  - Magma
  - Maple
  - Mathematica
  - MATLAB
  - SAS


--- .class &twocol_width

## More Software

*** =left width:45%

* *Machine &amp; Deep Learning* 
   - TensorFlow
   - Caffe
   - SciKit-Learn
   - SciKit-Image
   - Theano
   - Keras

* *Natural Language Processing (NLP)*
   - Natural Language Toolkit (NLTK)
   - Stanford NLP    


<span class="tiny">_[Python packages](https://go.lehigh.edu/python)_</span>


*** =right width:45%

* Bioinformatics
  - BamTools
  - BayeScan
  - bgc
  - BWA
  - FreeBayes
  - SAMTools
  - tabix
  - trimmomatic
  - Trinity
  - *barcode_splitter*
  - *phyluce* 
  - *VelvetOptimiser*


--- .class  &twocol_width

## More Software

*** =left width:35%

* Scripting Languages
  - R
  - Perl
  - Python
* Compilers
  - GNU
  - Intel
  - JAVA
  - PGI/NVIDIA HPC SDK
  - CUDA
* Parallel Programming
  - MVAPICH2
  - MPICH
  - OpenMPI


*** =right width:65%

* Libraries
  - BLAS/LAPACK/GSL/SCALAPACK
  - Boost
  - FFTW
  - Intel MKL
  - HDF5
  - NetCDF
  - METIS/PARMETIS
  - PetSc
  - QHull/QRupdate
  - SuiteSparse
  - SuperLU

--- .class &twocol_width
 
## More Software

*** =left width:45%

* Visualization Tools
  - Atomic Simulation Environment 
  - Avogadro
  - GaussView
  - GNUPlot
  - PWGui
  - PyMol
  - RDKit
  - VESTA
  - VMD
  - XCrySDen

*** =right width:45%

* Other Tools
  - Artleys Knitro
  - ROOT
  - CMake
  - GNU Parallel
  - Lmod
  - *Numba*
  - Scons
  - SPACK
  - *MD Tools*
     * BioPython
     * CCLib
     * MDAnalysis

--- .class &twocol_width

## Open OnDemand Applications

*** =left width:45%

* Jupyter Notebooks
* RStudio
* Ansys Workbench
* Abaqus CAE
* GNU Octave
* Maple
* Mathematica
* MATLAB
* SAS
* VMD
* Virtual Desktops (MATE and XFCE)

*** =right width:45%

* Visualization 
  * ASE
  * Avogadro
  * Blender
  * Gabedit
  * Gauss View
  * Paraview
  * PWGui
  * PyMol
  * VESTA
  * XCrySDen

--- .class

## Using your own Software?


* You can always install a software in your home directory
   - [SPACK](https://spack.readthedocs.io) is an excellent package manager that can even create module files
* `Stay compliant with software licensing`
* Modify your .bashrc/.tcshrc to add software to your path, OR
* create a module and dynamically load it so that it doesn&#39;t interfere 
 with other software installed on the system
  - e.g. You might want to use openmpi instead of mvapich2 
  - the system admin may not want install it system wide for just one user
* Add the directory where you will install the module files to the variable 
  MODULEPATH in .bashrc/.tcshrc
```{sh mymodule,eval=FALSE}
# My .bashrc file
export MODULEPATH=${MODULEPATH}:/home/alp514/modulefiles
```

--- .class

## Module File Example

<img width = '900px' src = 'assets/img/mcr.png'>


--- .class &twocol_width

## Cluster Environment

* A cluster is a group of computers (nodes) that works together closely

*** =left width:30%

* Two types of nodes
   - Head/Login Node
   - Compute Node

* Multi-user environment

* Each user may have multiple jobs running simultaneously

*** =right width:65%

<img width = '640px' src = 'assets/img/solnetwork.png'>


--- .class #id

## How to run jobs

* All compute intensive jobs are scheduled
* Write a script to submit jobs to a scheduler
  - need to have some background in shell scripting (bash/tcsh)
* Need to specify
   - Resources required (which depends on configuration)
       - number of nodes
       - number of processes per node
       - memory per node
   - How long do you want the resources
       - have an estimate for how long your job will run
   - Which queue to submit jobs
       - SLURM uses the term _partition_ instead of _queue_


--- .class #id

## Scheduler &amp; Resource Management

* A software that manages resources (CPU time, memory, etc) and schedules job execution
   - Sol: Simple Linux Utility for Resource Management (SLURM)
   - Others:  Portable Batch System (PBS)
          - Scheduler: Maui
          - Resource Manager: Torque
          - Allocation Manager: Gold

* A job can be considered as a user’s request to use a certain amount of resources for a certain amount of time

* The Scheduler or queuing system determines
    - The order jobs are executed
    - On which node(s) jobs are executed



--- .class #id &twocol_width
 
## Job Scheduling

*** =left width:50%

* Map jobs onto the node-time space
    - Assuming CPU time is the only resource

* Need to find a balance between
    - Honoring the order in which jobs are received
    - Maximizing resource utilization


*** =right width:45%

<img width = '440px' src = 'assets/img/JobSchedule-1.png'>


--- .class #id &twocol_width
 
## Backfilling

*** =left width:50%
* A strategy to improve utilization
   - Allow a job to jump ahead of others when there are enough idle nodes
   - Must not affect the estimated start time of the job with the highest priority

*** =right width:45%

<img width = '440px' src = 'assets/img/JobSchedule-2.png'>



--- .class #id &twocol_width
 
## How much time must I request

* Ask for an amount of time that is
    - Long enough for your job to complete
    - As short as possible to increase the chance of backfilling

*** =left width:45%

<img width = '360px' src = 'assets/img/JobSchedule-3.png'>

*** =right width:45%

<img width = '360px' src = 'assets/img/JobSchedule-4.png'>


--- .class #id .small

## Available Queues

| Partition Name | Max Runtime in hours | Max SU consumed node per hour | 
|:----------:|:--------------------:|:--------------------:|
| lts | 72 | 18 |
| lts-gpu | 72 | 20 |
| im1080 | 48 | 20 | 
| im1080-gpu | 48 | 24 |
| eng | 72 | 22 |
| eng-gpu | 72 | 24 |
| engc | 72 | 24 |
| himem | 72 | 48 |
| enge/engi/chem/health | 72 | 36 |
| im2080 | 48 | 28 |
| im2080-gpu | 48 | 36
| debug | 1 | 16 |


--- .class #id

## How much memory can or should I use per core?

* The amount of installed memory less the amount that is used by the operating system and other utilities

* A general rule of thumb on most HPC resources: leave 1-2GB for the OS to run. 

| Cluster | Partition | Max Memory/core (GB) | Recommended Memory/Core (GB) |
|:-------:|:---------:|:--------------------:|:----------------------------:|
| Sol | lts | 6.4 | 6.2 |
|     | eng/im1080/im1080-gpu/enge/engi/im2080/im2080-gpu/chem/health | 5.3 | 5.1 |
|     | engc | 2.66 | 2.4 |
|     | debug | 4 | 3.8 |

*  <span class="alert">if you need to run a single core job that requires 10GB memory in the eng partition, you need to request 2 cores even though you are only using
         1 core.</span>  

--- .class #id .small

## Useful SBATCH Directives

| SLURM Directive | Description |
|:---------------:|:-----------:|
| #SBATCH --partition=queuename | Submit job to the <em>queuename</em> partition. |
| #SBATCH --time=hh:mm:ss | Request resources to run job for <em>hh</em> hours, <em>mm</em> minutes and <em>ss</em> seconds. |
| #SBATCH --nodes=m | Request resources to run job on <em>m</em> nodes. |
| #SBATCH --ntasks-per-node=n | Request resources to run job on <em>n</em> processors on each node requested. |
| #SBATCH --ntasks=n | Request resources to run job on a total of <em>n</em> processors. |
| #SBATCH --job-name=jobname | Provide a name, <em>jobname</em> to your job. |
| #SBATCH --output=filename.out | Write SLURM standard output to file filename.out. |
| #SBATCH --error=filename.err | Write SLURM standard error to file filename.err. |
| #SBATCH --mail-type=events | Send an email after job status events is reached. |
| | events can be NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME_LIMIT(_90,80) |
| #SBATCH --mail-user=address | Address to send email. |
| #SBATCH --account=mypi | charge job to the __mypi__ account |


--- .class #id .small

## Useful SBATCH Directives (contd)

| SLURM Directive | Description |
|:---------------:|:-----------:|
| #SBATCH --qos=nogpu | Request a quality of service (qos)  for the job in `imlab`, `engc` partitions. |
| | Job will remain in queue indefinitely if you do not specify qos |
| #SBATCH --gres=gpu:# | Specifies number of gpus requested in the gpu partitions |
| | You can request 1 or 2 gpus with a minimum of 1 core or cpu per gpu | 

* SLURM can also take short hand notation for the directives

| Long Form | Short Form |
|:---------:|:----------:|
| --partition=queuename | -p queuename |
| --time=hh:mm:ss | -t hh:mm:ss |
| --nodes=m | -N m |
| --ntasks=n | -n n |
| --account=mypi | -A mypi |
| --job-name=jobname | -J jobname |
| --output=filename.out | -o filename.out |


--- .class #id

## SBATCH Filename Patterns

* sbatch allows for a filename pattern to contain one or more replacement
  symbols, which are a percent sign "%" followed by a letter (e.g. %j). 

| Pattern | Description |
|:-------:|:-----------:|
| %A |    Job array&#39;s master job allocation number. |
| %a |    Job array ID (index) number. |
| %J |    jobid.stepid of the running job. (e.g. "128.0") |
| %j |    jobid of the running job. |
| %N |    short hostname. This will create a separate IO file per node. |
| %n |    Node identifier relative to current job (e.g. "0" is the first node of the running job) This will create a separate IO file per node. |
| %s |    stepid of the running job. |
| %t |    task identifier (rank) relative to current job. This will create a separate IO file per task. |
| %u |    User name. |
| %x |    Job name. |


--- .class #id .pbs

## Useful SLURM environmental variables


| SLURM Command | Description |
|:-------------:|:-----------:|
| SLURM_SUBMIT_DIR | Directory where the <code>qsub</code> command was executed |
| SLURM_JOB_NODELIST | Name of the file that contains a list of the HOSTS provided for the job |
| SLURM_NTASKS | Total number of cores for job |
| SLURM_JOBID | Job ID number given to this job |
| SLURM_JOB_PARTITION | Queue job is running in |
| SLURM_JOB_NAME | Name of the job. |

--- .class #id

## Basic Job Manager Commands

* Submission
* Monitoring
* Manipulating
* Reporting

--- .class 

## Job Types

* Interactive Jobs
  - Set up an interactive environment on compute nodes for users
  - Will log you into a compute node and wait for your prompt
  - Purpose: testing and debugging code. __Do not run jobs on head node!!!__
      * All compute node have a naming convention __sol-[a,b,c]###__
      * head node is __sol__
* Batch Jobs
   - Executed using a batch script without user intervention
       - Advantage: system takes care of running the job
       - Disadvantage: cannot change sequence of commands after submission
   - Useful for Production runs
   - Workflow: write a script -> submit script -> take mini vacation ->
   analyze results

--- .class #id

## Job Types: Interactive

   - SLURM:  Use `srun` command with SBATCH Directives followed by `--pty /bin/bash`
      ```{bash eval=F}
      srun --time=<hh:mm:ss> --nodes=<# of nodes> --ntasks-per-node=<# of core/node> -p <queue name> --pty /bin/bash
      ```
       * If you have `soltools` module loaded, then use `interact` with at
   least one SBATCH Directive
           - `interact -t 20` [Assumes `-p lts -n 1 -N 18`]
   - Run a job interactively replace `--pty /bin/bash --login` with the
     appropriate command. 
       ```{bash eval=F}
       srun -t 20 -n 1 -p im1080 --qos=nogpu $(which lammps) -in in.lj -var x 1 -var n 1
       ```
       - Default values are 3 days, 1 node, 18 tasks per node and lts
   partition

--- .class #id

## Job Types: Batch 

* Workflow: write a script -> submit script -> take mini vacation -> analyze
  results
* Submitting Batch Jobs
   ```{bash eval=F}
   sbatch filename
   ```

* `sbatch` can take the options for `#SBATCH` as command line arguments
   ```{bash eval=F}
   sbatch --time=1:00:00 --nodes=1 --ntasks-per-node=18 -p lts filename
   ```

--- .class #id .big

## Minimal submit script for Serial Jobs


```{bash eval=FALSE}
#!/bin/bash
#SBATCH --partition=lts
#SBATCH --time=1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --job-name myjob

cd ${SLURM_SUBMIT_DIR}
./myjob < filename.in > filename.out

```

--- .class #id .big

## Minimal submit script for MPI Job


```{bash eval=FALSE}
#!/bin/bash
#SBATCH --partition=lts
#SBATCH --time=1:00:00
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=20
## For --partition=imlab, 
###  use --ntasks-per-node=22
### and --qos=nogpu
#SBATCH --job-name myjob

module load mvapich2

cd ${SLURM_SUBMIT_DIR}
srun ./myjob < filename.in > filename.out

exit
```

--- .class #id .big 

## Minimal submit script for OpenMP Job

```{bash eval=FALSE}
#!/bin/tcsh
#SBATCH --partition=eng
# Directives can be combined on one line
#SBATCH --time=1:00:00 --nodes=1 --ntasks-per-node=22
#SBATCH --job-name myjob

cd ${SLURM_SUBMIT_DIR}
# Use either
setenv OMP_NUM_THREADS 22
./myjob < filename.in > filename.out

# OR
OMP_NUM_THREADS=22 ./myjob < filename.in > filename.out

exit
```


--- .class #id .big

## Minimal submit script for LAMMPS GPU job

```{bash eval=FALSE}
#!/bin/tcsh
#SBATCH --partition=im1080-gpu
# Directives can be combined on one line
#SBATCH --time=1:00:00
#SBATCH --nodes=1
# 1 CPU can be be paired with only 1 GPU
# 1 GPU can be paired with all 24 CPUs
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:1
# Need both GPUs, use --gres=gpu:2
#SBATCH --job-name myjob

cd ${SLURM_SUBMIT_DIR}
# Load LAMMPS Module
module load lammps/17nov16-gpu
# Run LAMMPS for input file in.lj
srun $(which lammps) -in in.lj -sf gpu -pk gpu 1 gpuID ${CUDA_VISIBLE_DEVICE}

exit
```

--- .class #id

## Monitoring &amp; Manipulating Jobs


| SLURM Command | Description |
|:-----------:|:-----------:|:-------------:|
| squeue | check job status (all jobs) |
| squeue -u username | check job status of user <em>username</em> |
| squeue --start | Show <strong>estimated</strong> start time of jobs in queue |
| scontrol show job jobid | Check status of your job identified by <em>jobid</em> |
| scancel jobid | Cancel your job identified by <em>jobid</em> |
| scontrol hold jobid | Put your job identified by <em>jobid</em> on hold |
| scontrol release jobid | Release the hold that <strong>you put</strong> on <em>jobid</em> |


--- .class #id

## Need to run multiple jobs in sequence?

* In sequence or serially
    * Option 1: Submit jobs as soon as previous jobs complete
    * Option 2: [Submit jobs with a dependency](https://confluence.cc.lehigh.edu/x/FqH0BQ#SLURM-SubmittingDependencyjobs)
    ```{bash eval=F}
    sbatch --dependency=afterok:<JobID> <Submit Script>
    ````

* In parallel, use [GNU Parallel](https://confluence.cc.lehigh.edu/x/B6b0BQ)


--- .class #id .small

## Additional Help &amp; Information

* Issue with running jobs or need help to get started: 
  * Open a help ticket: <http://lehigh.edu/help>
* More Information
  * [Research Computing] (https://researchcomputing.lehigh.edu)
  * [Research Computing Wiki](https://go.lehigh.edu/rcwiki)
  * [Research Computing Training](https://go.lehigh.edu/hpcseminars)
* Subscribe
     * HPC Training Google Groups: <mailto:hpctraining-list+subscribe@lehigh.edu>
     * Research Computing Mailing List: <https://lists.lehigh.edu/mailman/listinfo/hpc-l>
* My contact info
  * eMail:  <alp514@lehigh.edu>
  * Tel: (610) 758-6735 
  * Location: Room 296, EWFM Computing Center ( Remote in Fall 2020)
  * [My Schedule] (https://www.google.com/calendar/embed?src=alp514%40lehigh.edu&ctz=America/New_York )


 
