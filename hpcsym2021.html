<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Research Computing</title>
    <meta charset="utf-8" />
    <meta name="author" content="Library &amp; Technology Services" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <link rel="stylesheet" href="addons/lehigh.css" type="text/css" />
    <link rel="stylesheet" href="addons/lehigh-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Research Computing
## @ Lehigh University
### Library &amp; Technology Services
### <a href="https://researchcomputing.lehigh.edu" class="uri">https://researchcomputing.lehigh.edu</a>

---

class: inverse, middle

# Welcome






---

# Research Computing Steering Committee (RCSC)

* __Co-chairs__ 
   - Wonpil Im (CAS)
   - Ed Webb (RCEAS)

* __Members__
   - Ganesh Balasubramanian (RCEAS)
   - Brian Chen (RCEAS)
   - Ben Felzer (CAS)
   - Lisa Fredin (CAS)
   - Alex Pacheco (LTS)
   - Srinivas Rangarajan (RCEAS)
   - Rosi Reed (CAS)
   - Seth Richards-Shubik (COB)
   - Yue Yu (CAS)



---

# Research Computing Steering Committee (RCSC)

* __Co-chairs__  
   - Wonpil Im (CAS)
   - __&lt;span style="color:red;"&gt;Ed Webb (RCEAS)&lt;/span&gt;__

* __Members__
   -  __&lt;span style="color:red;"&gt;Ganesh Balasubramanian (RCEAS)&lt;/span&gt;__
   - Brian Chen (RCEAS)
   - Ben Felzer (CAS)
   -  __&lt;span style="color:red;"&gt;Lisa Fredin (CAS)&lt;/span&gt;__
   -  __&lt;span style="color:red;"&gt;Alex Pacheco (LTS)&lt;/span&gt;__
   -  __&lt;span style="color:red;"&gt;Srinivas Rangarajan (RCEAS)&lt;/span&gt;__
   - __&lt;span style="color:red;"&gt;Rosi Reed (CAS)&lt;/span&gt;__
   - Seth Richards-Shubik (COB)
   - Yue Yu (CAS)



---
class: inverse,middle

.center[
# Celebration!
]

--

.right[
## with some history
]



---
class: inverse, middle, center

# HAWK


---
# Symposium Schedule


| Time | Session |
|:----:|:-------|
| 1:00 - 1:15| 	Welcome |
| 1:15- 1:30| 	Overview of Sol and Hawk |
| 1:30 - 2:00| 	Huaiqian You, Yu Lab, Department of Mathematics |
| 2:00 - 2:30| 	Yeol Kyo Choi, Im Lab, Department of Biological Sciences |
| 2:30 - 3:00| 	Account &amp; Allocations |
| 3:00 - 4:00| 	Lehigh Leadership's vision for HPC |
| 4:00 - 4:30| 	Christopher Rzepa, Rangarajan Lab, Department of Chemical &amp; Biomolecular Engineering |
| 4:30 - 5:00| 	New Features on Sol &amp; Hawk, Upcoming Events, Future Plans |
| 5:00 - 5:30| 	Open Session, Q &amp; A |


???

* Huaiqian You, An Asymptotically Compatible Treatment for Traction Loading in Peridynamic Fracture.

* Yeol Kyo Choi, CHARMM-GUI Polymer Builder for Modeling and Simulation of Synthetic Polymers

* Chris Rzepa, Towards Data-Driven Structure-Property Relations for Predicting Adsorption Entropy in Siliceous Zeolites

 



---
# Sol



- Lehigh&amp;#39;s Shared High Performance Computing Cluster 
  - built by investments from Provost and Faculty.
  - 9 nodes&lt;sup&gt;a&lt;/sup&gt;, dual 10-core Intel Xeon E5-2650 v3 2.3GHz CPU, 128GB RAM.
  - 33 nodes, dual 12-core Intel Xeon E5-2670 v3 2.3Ghz CPU, 128GB RAM.
  - 14 nodes, dual 12-core Intel Xeon E5-2650 v4 2.3Ghz CPU, 64GB RAM.
  - 1 node, dual 8-core Intel Xeon 2630 v3 2.4GHz CPU, 512GB RAM.
  - 24 nodes, dual 18-core Intel Xeon Gold 6140 2.3GHz CPU, 192GB RAM.
  - 6 nodes, dual 18-core Intel Xeon Gold 6240 2.6GHz, 192GB RAM.
  - 2 nodes, dual 26-core Intel Xeon Gold 6230R, 2.1GHz, 384GB RAM
  - 72 nVIDIA GTX 1080 &amp; 48 nVIDIA RTX 2080TI GPU cards.
  - 2:1 oversubscribed Infiniband EDR (100Gb/s) interconnect fabric.
  - 21.97M core hours or service units (SUs) of computing available.
      - Only 1.40M from Provost investment available Lehigh researchers.
&lt;br /&gt;

.footnote[
a: 8 nodes invested by Provost available to all Lehigh researchers.
]


---

# Sol Configuration


&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Nodes &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Intel Xeon CPU Type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; CPU Speed (GHz) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU TFLOPS &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU TFLOPs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2650 v3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1024 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.7600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.570 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1576800 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 33 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2670 v3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 792 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4224 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 496 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25.3440 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15.934 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6937920 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2650 v4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 336 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6768 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2943360 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2640 v3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5632 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140160 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6140 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 864 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 48 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4608 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 528 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.4720 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.392 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7568640 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6240 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 216 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.3680 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1892160 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6230R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 768 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3264 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 911040 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 89 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2508 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13184 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 97.5104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 36.896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21970080 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



---

# Condo Investments

* Sustainable model for High Performance Computing at Lehigh.
* Faculty (Condo Investor) purchase compute nodes from grants to increase overall capacity of Sol.
* LTS will provide for four years or length of hardware warranty purchased,
   * System Administration, Power and Cooling, User Support for Condo Investments.
* Condo Investor
   * receives annual allocation equivalent to their investment for the length of
     investment,
   * can utilize allocations on all available nodes, including nodes from other Condo Investors,
   * allows idle cycles on investment to be used by other Sol users,
   * unused allocation will not rollover to the next allocation cycle,
* Annual Allocation cycle is Oct. 1 - Sep. 30.

---

# Condo Investors

* Dimitrios Vavylonis, Physics (1 node)
* Wonpil Im, Biological Sciences (37 nodes, 98 GPUs)
* Anand Jagota, Chemical Engineering (1 node)
* Brian Chen, Computer Science &amp; Engineering (3 nodes)
* Ed Webb &amp; Alp Oztekin, Mechanical Engineering (6 nodes)
* Jeetain Mittal &amp; Srinivas Rangarajan, Chemical Engineering (13 nodes, 16 GPUs)
* Seth Richards-Shubik, Economics (1 node)
* Ganesh Balasubramanian, Mechanical Engineering (7 nodes)
* Department of Industrial &amp; Systems Engineering (2 nodes)
* Paolo Bocchini, Civil and Structural Engineering (1 node)
* Lisa Fredin, Chemistry (6 nodes)
* Hannah Dailey, Mechanical Engineering (1 node)
* College of Health (2 nodes)
- Condo Investments: 81 nodes, 2348 CPUs, 120 GPUs, 20.57M SUs
- Lehigh Investments: 8 nodes, 160 CPUs,  1.40M  SUs
- Total SU on Sol after Condo Investments: 21.97M

???

.pull-left[![:scale 90%](assets/img/sol-condo.png)]

---

# Ceph Storage resource

* LTS provides various storage options for research and teaching.
* Some are cloud based and subject to Lehigh&amp;#39;s Cloud Policy.
* For research, LTS provides a 768TB storage system called [Ceph](https://go.lehigh.edu/ceph).
* Ceph 
    * based on the Ceph software,
    * in-house, built, operated and administered by Research Computing Staff,
        * located in the EWFM Data Center.
    * provides storage for Research Computing resources,
    * can be mounted as a network drive on Windows or CIFS on Mac and Linux.
        * [See Ceph FAQ](http://lts.lehigh.edu/services/faq/ceph-faq) for more details.
* Research groups can purchase a sharable project space @ &amp;#36;375/TB for 5 years,
    * can share project space with anyone with a Lehigh ID at no additional charge.



---
# Hawk

* Funded by [NSF Campus Cyberinfrastructure award 2019035](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2019035&amp;HistoricalAwards=false).
   - PI: Ed Webb (MEM).
   - co-PIs: Balasubramanian (MEM), Fredin (Chemistry), Pacheco (LTS), and Rangarajan (ChemE).
   - Sr. Personnel: Anthony (LTS), Reed (Physics), Rickman (MSE), and Tak&amp;#225;&amp;#269; (ISE). 
* Compute
  - 26 nodes, dual 26-core Intel Xeon Gold 6230R, 2.1GHz, 384GB RAM.
  - 4 nodes, dual 26-core Intel Xeon Gold 6230R, 1536GB RAM.
  - 4 nodes, dual 24-core Intel Xeon Gold 5220R, 192GB RAM, 8 nVIDIA Tesla T4.
* Storage
  - 7 nodes, single 16-core AMD EPYC 7302P, 3.0GHz, 128GB RAM, two 240GB SSDs (for OS).
  - Per node
      - 3x 1.9TB SATA SSD (for CephFS).
      - 9x 12TB SATA HDD (for Ceph).
* Production: **Feb 1, 2021**.


---



### Hawk

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Nodes &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Intel Xeon CPU Type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; CPU Speed (GHz) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU TFLOPS &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU TFLOPs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 26 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6230R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1352 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9984 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 56.2432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11843520 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6230R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 208 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6144 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.6528 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1822080 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 5220R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 192 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 768 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3008 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1681920 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 34 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1752 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 69.1968 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15347520 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

--

### Hawk and Lehigh&amp;#39;s Investment in Sol

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Nodes &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU TFLOPS &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU TFLOPs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1024 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.8880 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1401600 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1752 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 69.1968 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15347520 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Network Layout Sol, Hawk &amp;amp; Ceph

![:scale 100%](assets/img/sol_hawk.png)



---
class: inverse, middle

# Invited Talks

## Huaiqian You, _An Asymptotically Compatible Treatment for Traction Loading in Peridynamic Fracture_, Yu Lab, Department of Mathematics

## Yeol Kyo Choi, _CHARMM-GUI Polymer Builder for Modeling and Simulation of Synthetic Polymers_, Im Lab, Department of Biological Sciences

???

* Huaiqian You, An Asymptotically Compatible Treatment for Traction Loading in Peridynamic Fracture.

* Im Lab, need to check with Wonpil

* Chris Rzepa


---
class: inverse, middle

# Allocations

---

# Resource Partitioning

| Investigator | Compute (SU) | Storage (TB)|
|:------------:|:-------:|:-------:|
| Proposing Team | 7.67M | 85 |
| OSG | 3.07M | 5 |
| Lehigh General | 3.84M | 75 |
| Provost DF | 0.77M | 20 |
| LTS | | 30TB |

- 50% to proposing team including
  - 300K SUs allocated for educational, outreach and training (EOT).
- 20% of resources with Open Science Grid (Grant requirement)
- 5% of compute and 20TB to be distributed at the Provost’s discretion.
- 30TB to LTS’s R-Drive (provides all faculty up to 1TB of storage space)
- 25% of compute and 75TB available Lehigh community through XSEDE style proposal.
  - reviewed by Research Computing Allocation Review Committee (RCARC)
      - 10% (380K) for small allocations similar to StartUp Allocations on XSEDE.
      - 90% (3.45M) for large allocations similar to Research Allocations on XSEDE.

---

# Resource Management

- Do away with charges for computing on Sol and add 1.4M SUs from Sol.
- Create a Director's Discretionary Fund (DDF) from Lehigh General
  - to allow Manager of Research Computing to distribute compute and storage.
- Faculty PIs can request allocation based on their needs
   - Trial: for researchers who want to try out Sol/Hawk 
      - distributed at discretion of Manager Research Computing
   - StartUp: for researchers with small computing needs
      - proposals approved by at least one member of RCARC
   - Research (including Storage): for researchers with large computing or storage needs
      - proposals reviewed and approved by entire RCARC twice a year
   - Education: for courses, workshops and other education, outreach and training purposes   

---

# Total Resource

| Investigator | Compute (SU) | Storage (TB)| Sol Compute (SU) | Total Compute (SU) |
|:------------:|:------------:|:-----------:|:----------------:|:------------------:|
| Proposing Team | 7.64M | 80 | | 7.64M |
| DDF | 300K | 20 | 500K | 800K |
| OSG | 3.07M | 5 | | 3.07M |
| StartUp | 380K | 20 | 120K | 500K |
| Research | 3.45M | 40 | 550K | 4M |
| Provost DF | 770K | 20 | 230K | 1M |
| LTS R Drive| | 30TB | | | |

???

.pull-left[![:scale 90%](assets/img/sol-condo.png)]
.pull-right[![:scale 90%](assets/img/sol-condo-hawk.png)]

---

# Director's Discretionary Fund

- Trial Allocations: short term allocations designed for new users. 
  - Total Available: 300K SUs, 20TB.
  - Max 10K per PI, 1TB Storage.
  - 6 month duration.
  - Requirement: Short abstarct describing proposed work.
  - Progress report will be required for renewals or subsequent allocation request.
  - Apply anytime at https://go.lehigh.edu/hpcalloctrial

- Education Allocations: for courses, workshops and other EOT purposes
  - Total Available: 500K SUs
  - Up to 50K SUs per course, assuming 2 courses in Fall &amp; Spring.
  - 100K for REU programs.
  - 200K for workshops, and other Summer programs.
  - Apply at https://go.lehigh.edu/hpcallocedu 

---

# StartUp Allocations

- Maximum available: 500K SU.
- Max 25K per request.
- Max 2 startup allocations per PI.
- 1TB per PI (across all projects).
- Reviewed &amp; approved on a rolling basis by at least one allocation committee member.
- Requirement: short abstract with the description of the work.
- Can be renewed annually
- Renewal requires progress report, publication list, grants/ awards and list of graduated students if any.
- Apply anytime at https://go.lehigh.edu/hpcallocstartup

---

# Research Allocations

- For requests &gt; 25K SUs but &lt;300K SUs.
- Max allocation is 300K per PI at any given time.
- Requires a detailed proposal similar to XSEDE. Should contain
  - Abstract, Description of the project and how Hawk will be used.
  - If possible, justify requested allocations based on benchmarks results
  - Report of prior work done using Sol or Hawk
  - List of publications, presentations, awards and students graduated.  
- Reviewed and Approved semi annually by allocation committee.
- Allocations to begin on Jan1 and Jul 1 and last a year.
  - Call for proposals will be issued in May and Nov in the HPC mailing list
- Total time allocated is 4M annually or 2M for each semi annual request period.





---

# Storage Allocations

- Every PI will be allocated 1TB by the HPC Manager from the DDF and Lehigh StartUp allocation (40TB).
- HPC PIs who need more storage can request additional storage by justify needs with their compute allocation request.
- Non HPC PIs will need to submit a research proposal describing
   - Abstract and description of project.
   - Describe why other forms of storage are not suitable and any plans to backup data stored.
   - Plans to  move/save the data if project is not renewed/approved next year.
- Proposal reviewed and approved by allocation committee.
- Same timelines as Research Allocations

---

# Allocation Committee

| Name | College | 
|:----:|:-------:|
| Wonpil Im (co-chair) | CAS |
| Ed Webb (co-chair) | RCEAS |
| Ganesh Balasubramanian | RCEAS |
| Brian Chen | RCEAS |
| Ben Felzer | CAS |
| Lisa Fredin | CAS |
| Srinivas Rangarajan | RCEAS |
| Rosi Reed | CAS |
| Seth Richards-Shubik | CoB |
| Yue Yu | CAS |
| | CoH |
| | CoE |
| Alex Pacheco | LTS |


---

# Allocation Committe Responsibility

| Allocation Type | Max SUs | Max TB | Approval Authority | Request Window | Approval Timeline |
|:-----:|:----:|:----:|:----:|:----:|:----:|
| Trial | 10K | 1 | HPC | Rolling | 2-3 Business days |
| StartUp | 25K | 1 | One RCARC member | Rolling | 1 week |
| Trial/StartUp Renewals | 25K | 1 | Two RCARC members | Rolling | 2-3 weeks |
| Research/Storage | 300K | 5 | RCARC Committee | Every 6 months | Within a month of submission deadline |


---

# XSEDE Research Allocation Request

* Eligibility: faculty members and researchers, including postdoctoral researchers, at a U.S.-based institution.
  * Investigators with support from any funding source, not just NSF, are encouraged to apply.
  * an individual investigator may have only one active Research project at a time. 
  * PIs can request that other users be given accounts to use the allocation.
- Required Components
  * [Main Document](https://portal.xsede.org/allocations/research#requireddocs-main):
     * 10 pages for requests &amp;lt; 10M SUs.
     * 15 pages for requests &amp;ge; 10M SUs.
  * [Progress Report](https://portal.xsede.org/allocations/research#requireddocs-progress): 3 pages.
  * [Code Performance &amp;amp; Resource Costs](https://portal.xsede.org/allocations/research#requireddocs-codeperformance): 5 pages.
  * [Curriculum Vitae (CV)](https://portal.xsede.org/allocations/research#requireddocs-cv): 2 pages.
  * [References](https://portal.xsede.org/allocations/research#requireddocs-refs): No limit.
  * Optional - [Special Requirements](https://portal.xsede.org/allocations/research#requireddocs-reqs): 1 page.
  


---

# Main Document Guidance

  * [Scientific Background and Support](https://portal.xsede.org/allocations/research#maindoc-scientificbackground)
    * succinctly state the scientific objectives that will be facilitated by the allocation(s).
  * [Research Questions](https://portal.xsede.org/allocations/research#maindoc-researchquestions)
    * identify the specific research questions that are covered by the allocation request.
  * [Resource Usage Plan](https://portal.xsede.org/allocations/research#maindoc-resourceusage)
    * The bulk of the Main Document should focus on the resource usage plan and allocation request. 
    * **Inadequate justification for requested resources is the primary reason for most reduced or denied allocations.**
  * [Justifying Allocation Amounts](https://portal.xsede.org/allocations/research#maindoc-allocations)
    * combine quantitative parameters from the resource usage plan with basic resource usage cost information to calculate the allocation needed. 
    * tabulate and calculate the costs for each resource and resource type.
  * [Resource Appropriateness](https://portal.xsede.org/allocations/research#maindoc-resourceappropriateness)
    * briefly detail the rationale for selecting the requested resources.
  * [Disclosure of Access to Other Compute Resources](https://portal.xsede.org/allocations/research#maindoc-accesstoresources)



---
class: inverse, middle

# Lehigh Leadership


## How does our community continue to grow in concert with the leadership vision for Lehigh University over the next 5-10 years?



---
class: inverse, middle

# Invited Talk

## Christopher Rzepa, _Towards Data-Driven Structure-Property Relations for Predicting Adsorption Entropy in Siliceous Zeolites_, Rangarajan Lab, Department of Chemical &amp; Biomolecular Engineering

---
class: inverse, middle

# New Features on Sol &amp; Hawk
# Upcoming Events
# Outlook 

---

# Open On Demand Web Portal

- Open, Interactive HPC via the Web. 
    - Easy to use, plugin-free, web-based access to supercomputers,
    - File Management,
    - Command-line shell access,
    - Job management and monitoring, and 
    - Various Interactive Applications.
         - Jupyter Notebooks
         - RStudio Servers
         - Virtual Desktop Interface
         - MATLAB
         - Visualization Tools
         - Tailored apps for course use
    - Developed by [Ohio Supercomputing Center](https://openondemand.org/) with support from NSF
         - SI2-SSE-1534949 and CSSI-Software-Frameworks-1835725
- https://hpcportal.cc.lehigh.edu
     - Lehigh IP or VPN required.
 

---

# Open On Demand Web Portal

![:scale 90%](assets/img/OOD.png)


---

# Singularity Containers

* &lt;em&gt;Containers for Science&lt;/em&gt;
   - Developed by Greg Kurtzer at Lawrence Berkeley National Lab.
* Not based on Docker, but can directly import/run Docker images.
* HPC oriented - can run mpi applications across multiple nodes.
* Can run images in user space.
* BYOS - build on your local system, run on Sol, Hawk and most Supercomputing Centers.


--

* Open On Demand Apps
   - Virtual Desktop Interface
   - Desktop Environment to launch GUI Apps
   - RStudio Server
* Ansys 2020 R2
* Bioinformatics Application - Cactus 
   

---

# Application tuning for Hybrid Architectures

* Using [SPACK](https://spack.io) to install most applications.
* Applications are optimized for
    - Ivybridge (Pavo - dev cluster), Haswell and Skylake CPU Architectures.
    - MPICH (Hawk/GbE) and MVAPICH2 (Sol,Pavo/Infiniband)
* Application delivered using LMOD (replaces TCL environment modules).
   - Not all applications visible by default.
   - Dynamically swap applications depending on architecture and mpi implementation.
   - By default, application built using 
      - Intel and MPICH visible on Hawk (Skylake)
      - Intel and MVAPICH2 visible on Sol (Haswell &amp; Skylake) and Pavo (Ivybridge)
* For help with an application not working as expected, specify
   - application name and version
   - partition application was running on


---

# Upcoming HPC Seminars

* Fridays from 2:00PM - 4:00PM.
  - Research Computing Resources at Lehigh (Feb. 5)
  - Linux: Basic Commands &amp; Environment (Feb. 12).
  - Using SLURM scheduler on Sol (Feb. 19).
  - Introduction to Open OnDemand (Feb. 26).
  - Python Programming (Mar. 5).
  - Build/Bring Your Own Software (Mar. 12)
  - Python Data Structure (Mar. 19)
  - Creating `\(\LaTeX\)` Documents using Overleaf (Mar. 26)
  - R Programming (Apr. 2).
  - Data Visualization with Python (Apr. 9).
  - Data Visualization with R (Apr. 16).
  - Machine Learning (Apr. 23).
  - Shiny Apps in R (Apr. 30).
* Register at https://go.lehigh.edu/hpcseminars


---

# Summer Activities 

* Programming Workshops (0.5 - 2 days for each topic)
   - C/C++
   - Fortran
   - OpenMP
   - MPI
   - OpenACC
   - CUDA
* HPC Seminars


--

* Volunteer as TA's
* Interested in hosting a workshop, adding to or assisting with above topics


---

# LTS Seminars

* Recordings of Winter Tech Talk Series
   - https://go.lehigh.edu/ltsseminars
* Spring Seminars to be announced soon

* Upcoming CITL events: https://citl.lehigh.edu/events

* Friends of Library event on April 5 at 5PM
   - Prof. Osagie K. Obasogie, Haas Distinguished Chair and Professor of Bioethics from University of California, Berkeley, will discuss issues of diversity and inclusion and law as these relate to Bioethics and policy 



---

# Future Plans for Growth

* EWFM Data Center.
    * Add Row 4 for further expansion of Sol.
         * Requests from various faculty will consume all available space and power in row 2 within 6 months.
    * Power and Cooling for Row 4.
    * Upgrade from 15KW/rack in Row 2 to 30KW/rack as nodes are decommissioned.

* Develop Indirect Cost Recovery plans to replace Hawk in 2024-25

* Apply for Cluster Grants
    * NIH S10 within the next two years.
    * NSF MRI in 2023-24 to replace Hawk.
    * Any infrastructure grants we missed?

 


---

# More Information

- Investing in Sol
  - Contact Alex Pacheco or Steve Anthony
* More Information
  * [Research Computing](https://go.lehigh.edu/hpc)
  * [Research Computing Seminars](https://go.lehigh.edu/hpcseminars)
  * [Accounts &amp; Allocations](https://go.lehigh.edu/hpcusagepolicy)
  * [Condo Program](https://confluence.cc.lehigh.edu/x/EgL5Bg)
  * [Proposal Assistance](https://confluence.cc.lehigh.edu/x/FgL5Bg)
  * [Data Management Plans](http://libraryguides.lehigh.edu/researchdatamanagement)
  * Subscribe
      * [Research Computing Mailing List](https://lists.lehigh.edu/mailman/listinfo/hpc-l)
      * [HPC Training Google Groups](mailto:hpctraining-list+subscribe@lehigh.edu)

---

# Thanks to

* ORSP Staff - Heather Messina and Andrew Kline
* Purchasing Services - Jane Altemose and Nick Rose
* TIO Operations Staff - Lori Carroll and Ryan Pacala
* Other TIO/LTS Staff - Munroe Sellog, Jeff Deschler, Keith Erekson, James Monek, Ilena Key
* Past and Present University Leadership - Pat Farrell, Bruce Taggart, Greg Reihman, Bob Flowers, Steve DeWeerth and Nathan Urban  
* All HPC Users


--

* In publications, reports, and presentations that utilize Sol, Hawk and Ceph, please acknowledge Lehigh University using the following statement:

&lt;em&gt;"Portions of this research were conducted on Lehigh University's Research Computing infrastructure partially supported by NSF Award 2019035"&lt;/em&gt;



---
class: inverse middle

# Thank You!
# Questions?


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script src="addons/imgscale.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:10",
"yolo": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
