<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to HPC</title>
    <meta charset="utf-8" />
    <meta name="author" content="Library &amp; Technology Services" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="addons/lehigh.css" type="text/css" />
    <link rel="stylesheet" href="addons/lehigh-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to HPC
## Research Computing @ Lehigh University
### Library &amp; Technology Services
### <a href="https://researchcomputing.lehigh.edu" class="uri">https://researchcomputing.lehigh.edu</a>

---

class: inverse, middle



# What is HPC? 
# Who uses it?

---

# Why use HPC?

*  HPC may be the only way to achieve computational goals in a given amount of time

     * **Size**: Many problems that are interesting to scientists and engineers cannot fit on a PC usually because they need more than a few GB of RAM, or more than a few hundred GB of disk.

     * **Speed**: Many problems that are interesting to scientists and engineers would take a very long time to run on a PC: months or even years; but a problem that would take a month on a PC might only take a few hours on a supercomputer.

&lt;br /&gt;
.center[
&lt;img src="assets/img/irma-at201711_ensmodel_10-5PM.gif" alt="Irma Ensemble
Model" width="200px"&gt;
&lt;img src="assets/img/irma-at201711_model-10-5PM.gif" alt="Irma High Probability"
width="200px"&gt;
&lt;img src="assets/img/jose-at201712_ensmodel.gif" alt="Jose Ensemble Model"
width="200px"&gt;
&lt;img src="assets/img/jose-euro-sep11.png" alt="Jose Euro Model Track Forecast"
width="210px"&gt;
]

---

#  Parallel Computing

* many calculations are carried out simultaneously.
- based on principle that large problems can often be divided into smaller ones, which are then solved in parallel.
*  Parallel computers can be roughly classified according to the level at which the hardware supports parallelism.
    * Multicore computing
    * Symmetric multiprocessing
    * Distributed computing
    * Grid computing
    * General-purpose computing on graphics processing units (GPGPU)

---

# What does HPC do?


.pull-left[
* Simulation of Physical Phenomena
     * Storm Surge Prediction
     *  Black Holes Colliding
     *  Molecular Dynamics
- Data analysis and Mining
     -  Bioinformatics
     -  Signal Processing
     -  Fraud detection
* Visualization
.center[
&lt;img src="assets/img/Isaac-Storm-Surge.jpeg" alt="Isaac Storm Surge" width="200px"&gt;
&lt;img src="assets/img/Colliding-Black-Holes.jpeg" alt="Colliding Black Holes" width="170px"&gt;]
]

.pull-right[
* Design
     *  Supersonic ballute
     *  Boeing 787 design
     *  Drug Discovery
     *  Oil Exploration and Production
     *  Automotive Design
     *  Art and Entertainment
.center[
&lt;img src="assets/img/Molecular-Dynamics.jpeg" alt="Molecular Dynamics" width="180px"&gt;
&lt;img src="assets/img/Plane-Design.jpg" alt="Plane Design" width="180px"&gt;]
]



---

# HPC by Disciplines

* Traditional Disciplines
    * Science: Physics, Chemistry, Biology, Material Science
    * Engineering: Mechanical, Structural, Civil, Environmental, Chemical

- Non Traditional Disciplines
    - Finance
        - Preditive Analytics
        - Trading
    - Humanities
         - Culturomics or cultural analytics: study human behavior and cultural trends through quantitative analysis of digitized texts, images and videos.


---
class: inverse, middle

# Research Computing Resources


---

# Sol: Lehigh&amp;#39;s Shared HPC Cluster




&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Nodes &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Intel Xeon CPU Type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; CPU Speed (GHz) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU TFLOPS &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU TFLOPs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2650 v3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1024 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.7600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.570 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1576800 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 33 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2670 v3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 792 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 62 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4224 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 496 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25.3440 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15.934 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6937920 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2650 v4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 336 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6768 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2943360 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E5-2640 v3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5632 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140160 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6140 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 864 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 48 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4608 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 528 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 41.4720 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.392 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7568640 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6240 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 216 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.3680 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1892160 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6230R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 768 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3264 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 911040 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 89 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2508 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13184 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 97.5104 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 36.896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21970080 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
- built by investments from Provost&lt;sup&gt;a&lt;/sup&gt; and Faculty.
- 87 nodes interconnected by 2:1 oversubscribed Infiniband EDR (100Gb/s) fabric.
- Only 1.40M SUs from Provost investment available to Lehigh researchers.

.footnote[
a: 8 Intel Xeon E5-2650 v3 nodes invested by Provost.
]


---
# Hawk

* Funded by [NSF Campus Cyberinfrastructure award 2019035](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2019035&amp;HistoricalAwards=false).
   - PI: Ed Webb (MEM).
   - co-PIs: Balasubramanian (MEM), Fredin (Chemistry), Pacheco (LTS), and Rangarajan (ChemE).
   - Sr. Personnel: Anthony (LTS), Reed (Physics), Rickman (MSE), and Tak&amp;#225;&amp;#269; (ISE). 
* Compute
  - 26 nodes, dual 26-core Intel Xeon Gold 6230R, 2.1GHz, 384GB RAM.
  - 4 nodes, dual 26-core Intel Xeon Gold 6230R, 1536GB RAM.
  - 4 nodes, dual 24-core Intel Xeon Gold 5220R, 192GB RAM, 8 nVIDIA Tesla T4.
* Storage: 798TB (225TB usable)
  - 7 nodes, single 16-core AMD EPYC 7302P, 3.0GHz, 128GB RAM, two 240GB SSDs (for OS).
  - Per node
      - 3x 1.9TB SATA SSD (for CephFS).
      - 9x 12TB SATA HDD (for Ceph).
* Production: **Feb 1, 2021**.


---



### Hawk

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Nodes &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Intel Xeon CPU Type &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; CPU Speed (GHz) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU TFLOPS &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU TFLOPs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 26 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6230R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1352 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9984 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 56.2432 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11843520 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 6230R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 208 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6144 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.6528 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1822080 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gold 5220R &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 192 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 768 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.3008 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1681920 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 34 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1752 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 69.1968 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15347520 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

--

### Hawk and Lehigh&amp;#39;s Investment in Sol

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Nodes &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPUs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU Memory (GB) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; CPU TFLOPS &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; GPU TFLOPs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SUs &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1024 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.8880 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1401600 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1752 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16896 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 69.1968 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15347520 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 42 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 1912 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 17920 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 512 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 75.0848 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 8.10816 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #643700 !important;"&gt; 16749120 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Ceph Storage

* LTS provides various storage options for research and teaching.
* Some are cloud based and subject to Lehigh&amp;#39;s Cloud Policy.
* Research Computing provides a 1223TB (346TB) storage system called [Ceph](https://go.lehigh.edu/ceph).
* Including Hawk's storage, a combined 2019TB (572TB usable) Ceph storage system is available to Lehigh researchers..
* Ceph 
    * based on the Ceph software,
    * in-house, built, operated and administered by Research Computing Staff,
        * located in the EWFM Data Center.
    * provides storage for Research Computing resources,
    * can be mounted as a network drive on Windows or CIFS on Mac and Linux.
        * [See Ceph FAQ](http://lts.lehigh.edu/services/faq/ceph-faq) for more details.


---

# Sol 

.left-column[
![:scale 100%](assets/img/sol/20160627_153416.jpg)
]

.right-column[
![:scale 100%](assets/img/sol/resolve-launching-hawk-cover.jpg)
]


---

# Sol


.pull-left[ ![:scale 60%](assets/img/sol/20160509_133642.jpg) ]

.pull-right[![:scale 80%](assets/img/sol/20171011_104200.jpg)]



---

# Sol

.pull-left[![:scale 80%](assets/img/sol/20170301_112522.jpg)]

.pull-right[![:scale 80%](assets/img/sol/Lehigh-4181.jpg)]

---
class: inverse, middle

# Software


---

# Installed Software

.pull-left[

* [Chemistry/Materials Science](https://confluence.cc.lehigh.edu/x/3KX0BQ)
  - **CRYSTAL17** (Restricted Access)
  - **GAMESS**
  - Gaussian (Restricted Access)
  - **OpenMolcas**
  - **NWCHEM**
  - **Quantum Espresso**
  - **VASP** (Restricted Access)
* [Molecular Dynamics](https://confluence.cc.lehigh.edu/x/6qX0BQ)
  - **ESPResSo**
  - **GROMACS**
  - **LAMMPS**
  - **NAMD**

&lt;span class="tiny strong"&gt;__MPI enabled__&lt;/span&gt;
]

.pull-right[

* [Computational Fluid Dynamics](https://confluence.cc.lehigh.edu/x/BZFVBw)
  - Abaqus
  - Ansys
  - Comsol
  - **OpenFOAM**
  - OpenSees
* [Math](https://confluence.cc.lehigh.edu/x/1QL5Bg)
  - Artleys Knitro (node restricted)
  - GNU Octave
  - Gurobi
  - Magma
  - Maple
  - Mathematica
  - MATLAB]

---

# More Software

.pull-left[

* *Machine &amp;amp; Deep Learning* 
   - TensorFlow
   - SciKit-Learn
   - SciKit-Image
   - Theano
   - Keras

* *Natural Language Processing (NLP)*
   - Natural Language Toolkit (NLTK)
   - Stanford NLP    


&lt;span class="tiny"&gt;_[Python packages](https://go.lehigh.edu/python)_&lt;/span&gt;
]

.pull-right[

* [Bioinformatics](https://confluence.cc.lehigh.edu/x/y6X0BQ)
  - BamTools
  - BayeScan
  - bgc
  - BWA
  - FreeBayes
  - SAM Tools
  - tabix
  - trimmomatic
  - Trinity
  - *barcode_splitter*
  - *phyluce* 
  - VCF Tools
  - *VelvetOptimiser*]


---

#More Software

.pull-left[

* Scripting Languages
  - Julia
  - Perl
  - [Python](https://go.lehigh.edu/python)
  - [R](https://confluence.cc.lehigh.edu/x/5aX0BQ)
* [Compilers](https://confluence.cc.lehigh.edu/x/Sab0BQ)
  - GNU
  - Intel
  - JAVA
  - NVIDIA HPC SDK
  - CUDA
* [Parallel Programming](https://confluence.cc.lehigh.edu/x/Sab0BQ#Compilers-MPI)
  - MVAPICH2
  - MPICH
  - OpenMPI
]

.pull-right[

* Libraries
  - ARPACK/BLAS/LAPACK/GSL
  - FFTW/Intel MKL/Intel TBB
  - Boost
  - Global Arrays
  - HDF5
  - HYPRE
  - NetCDF
  - METIS/PARMETIS
  - MUMPS
  - PetSc
  - QHull/QRupdate
  - SuiteSparse
  - SuperLU
  - TCL/TK
]


---

# More Software

.pull-left[

* [Visualization Tools](https://confluence.cc.lehigh.edu/x/qan0BQ)
  - Atomic Simulation Environment 
  - Avogadro
  - Blender
  - Gabedit
  - GaussView
  - GNUPlot
  - GraphViz
  - Paraview
  - POV-RAY
  - PWGui
  - PyMol
  - RDKit
  - VESTA
  - VMD
  - XCrySDen
]

.pull-right[

* Other Tools
  - GNU Make/CMake
  - GDB/DDD
  - GIT
  - Intel Advisor/Inspector/Vtune Amplifier
  - [GNU Parallel](https://confluence.cc.lehigh.edu/x/B6b0BQ)
  - [Jupyter Lab/Notebooks](https://confluence.cc.lehigh.edu/x/G5JVBw)
  - [RStudio Server](https://confluence.cc.lehigh.edu/x/NpBVBw)
  - Scons
  - Singularity
  - SWIG
  - TMUX/GNU Screen
  - Valgrind/QCacheGrind
  - [Virtual Desktops](https://confluence.cc.lehigh.edu/x/g5BVBw)
]

---

# Accessing Resources

* Sol: accessible using ssh while on Lehigh&amp;#39;s network.
 
 ```bash
 ssh username@sol.cc.lehigh.edu
 ```
   * Windows PC require a SSH client such as [MobaXterm](https://mobaxterm.mobatek.net/) or [Putty](https://putty.org/).
   * Mac and Linux PC&amp;#39;s, ssh is built in to the terminal application. 
* Login to the ssh gateway to get on Lehigh&amp;#39;s network or connect to VPN first 
 
 ```bash
 ssh username@ssh.cc.lehigh.edu
 ```
 and then login to Sol using the above ssh command.
  *  Alternatively, use the following command while off campus.
  
  ```bash
  ssh -J username@ssh.cc.lehigh.edu username@sol.cc.lehigh.edu
  ```
  * [Configure MobaXterm to use the SSH Gateway](https://confluence.cc.lehigh.edu/x/JhH5Bg).


---

# Open OnDemand

- Open, Interactive HPC via the Web. 
    - Easy to use, plugin-free, web-based access to supercomputers,
    - File Management,
    - Command-line shell access,
    - Job management and monitoring, and 
    - Various Interactive Applications.
* NSF-funded project. 
    * SI2-SSE-1534949 and CSSI-Software-Frameworks-1835725,
    * Developed by [Ohio Supercomputing Center](https://openondemand.org/), 
    * Deployed at dozens of sites (universities, supercomputing centers).
- At Lehigh: https://hpcportal.cc.lehigh.edu.
    - Lehigh IP or VPN required.


---

# Open OnDemand

![:scale 90%](assets/img/OOD-Clusters.png)



---

# Open OnDemand

![:scale 90%](assets/img/OOD-Shell.png)

---
class: inverse, middle

# Scheduler Basics


---

# Cluster Environment

* A cluster is a group of computers (nodes) that works together closely.

.pull-left[

* Two types of nodes:
   - Head/Login Node,
   - Compute Node

* Multi-user environment.

* Each user may have multiple jobs running simultaneously.
]

.pull-right[
&lt;img width = '640px' src = 'assets/img/solnetwork.png'&gt;
]


---

# Scheduler &amp;amp; Resource Management

* A software that manages resources (CPU time, memory, etc) and schedules job execution.
- Simple Linux Utility for Resource Management (SLURM)
    - Scheduler
    - Resource Manager
    - Allocation Manager

* A job can be considered as a user’s request to use a certain amount of resources for a certain amount of time.

* The Scheduler or queuing system determines
    -  order jobs are executed, and
    -  which node(s) jobs are executed.


---

# How to run jobs

* All compute intensive jobs are scheduled.
- Write a script to submit jobs to a scheduler.
  - need to have some background in shell scripting (bash/tcsh).
* Have an understanding of 
   * Resources required (which depends on configuration)
       * number of nodes,
       * number of processes per node, and
       * memory required to run your job
   - Amount of time resources are required?
       - have an estimate for how long your job will run.
       - jobs have a max walltime of 2 or 3 days. 
             - can your jobs be restarted from a checkpoint.
   * Which partition to submit jobs?
       * SLURM uses the term _partition_ instead of _queue_.


---

# Available Queues on Sol

| Partition Name | Max Runtime in hours | Max SU consumed node per hour |
|:----------:|:--------------------:|:--------------------:|
| lts/lts-gpu | 72 | 19/20 |
| im1080/im1080-gpu | 48 | 20/24 | 
| eng/eng-gpu | 72 | 22/24 |
| engc | 72 | 24 |
| himem | 72 | 48 |
| enge/engi | 72 | 36 |
| im2080/im2080-gpu | 48 | 28/36 |
| im2080-gpu | 48 | 36 |
| chem/health | 48 | 36 |
| debug | 1 | 16 |
| hawkcpu | 72 | 52 |
| hawkmem | 72 | 52 |
| hawkgpu | 72 | 48 |
| infolab | 72 | 52 |


---

# How much memory?

* The amount of installed memory less the amount that is used by the operating system and other utilities.

* A general rule of thumb on most HPC resources: leave 1-2GB for the OS to run. 

| Partition | Max Memory/core (GB) | Recommended Memory/Core (GB) |
|:---------:|:--------------------:|:----------------------------:|
| lts | 6.4 | 6.3 |
| eng/im1080/enge/engi/im2080/chem/health | 5.3 | 5.2 |
| engc | 2.66 | 2.5 |
| himem | 32 | 31.8 |
| hawkcpu/infolab | 7.38 | 7.3 |
| hawkmem | 29.5 | 29.4 |
| hawkgpu | 4.0 | 3.9 |



*  &lt;code&gt;if you need to run a single core job that requires 10GB memory in the im1080 partition, you need to request 2 cores even though you are only using 1 core.&lt;/code&gt;  

???

| Partition | Max Memory/core (GB) | Recommended Memory/Core (GB) |
|:---------:|:--------------------:|:----------------------------:|
| hawk | 7.38 | 7.2 |
| hawk-tb | 29.54 | 29 | 
| hawk-gpu | 3.7 | 3.6 |


---

# Job Types

* Interactive Jobs:
  - Set up an interactive environment on compute nodes for users.
  - Will log you into a compute node and wait for your prompt.
  - Purpose: testing and debugging code. __Do not run jobs on head node!!!__
      * All compute node have a naming convention __sol-[a,b,c,d,e]###__ 
      * head node is __sol__.
* Batch Jobs:
   - Executed using a batch script without user intervention.
       - Advantage: system takes care of running the job.
       - Disadvantage: cannot change sequence of commands after submission.
   - Useful for Production runs.
   - Workflow: write a script -&gt; submit script -&gt; take mini vacation -&gt;
   analyze results.


---

# Job Types: Interactive

- Use `srun` command with SLURM Directives followed by `--pty /bin/bash`.
    * `srun --time=&lt;hh:mm:ss&gt; --nodes=&lt;# of nodes&gt; --ntasks-per-node=&lt;# of core/node&gt; -p &lt;queue name&gt; --pty /bin/bash`
    * If you have `soltools` module loaded, then use `interact` with at least one SLURM Directive.
        * `interact -t 20` [Assumes `-p lts -n 1 -N 1`]
* Run a job interactively replace `--pty /bin/bash --login` with the appropriate command. 
    * For e.g. `srun -t 1 -n 52 -p hawkcpu $(which lmp) -in in.lj -var x 1 -var n 10000`
    * Default values are 3 days, 1 node, 1 task and lts partition.



---

# Job Types: Batch 

* Workflow: write a script -&gt; submit script -&gt; take mini vacation -&gt; analyze
  results.

* Batch scripts are written in bash, tcsh, csh or sh.

* Add SLURM directives after the shebang line but before any shell commands.
&lt;pre&gt;
   #!/bin/bash
   #SBATCH --time=1:00:00
   #SBATCH --nodes=1
   #SBATCH --ntasks-per-node=20
   #SBATCH -p lts
   source /etc/profile.d/zlmod.sh
   cd ${SLURM_SUBMIT_DIR}
&lt;/pre&gt;
* Submitting Batch Jobs:
&lt;pre&gt;
   sbatch myjob.slr
&lt;/pre&gt;
* `sbatch` can take `#SBATCH DIRECTIVES` as command line arguments.
&lt;pre&gt;
   sbatch --time=1:00:00 --nodes=1 --ntasks-per-node=20 -p lts myjob.slr
&lt;/pre&gt;


---

# Submit script for Serial Jobs


&lt;pre&gt;
#!/bin/bash
#SBATCH --partition=lts
#SBATCH --time=1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH -J myjob

source /etc/profile.d/zlmod.sh
cd ${SLURM_SUBMIT_DIR}
#SBATCH --mail-type=ALL &lt;--- this is a comment not a SLURM DIRECTIVE
./myjob &lt; filename.in &gt; filename.out

# Example
/share/Apps/examples/simple_jobs/laplace_serial &lt;&lt; EOF
400
EOF


&lt;/pre&gt;


---

# Submit script for OpenMP Job

&lt;pre&gt;
#!/bin/tcsh
#SBATCH --partition=im1080
# Directives can be combined on one line
#SBATCH --time=1:00:00 --nodes=1 --ntasks-per-node=20
#SBATCH --job-name=myjob

source /etc/profile.d/zlmod.csh
cd ${SLURM_SUBMIT_DIR}
# Use either
setenv OMP_NUM_THREADS 20
./myjob &lt; filename.in &gt; filename.out

# OR
OMP_NUM_THREADS=20 ./myjob &lt; filename.in &gt; filename.out

# Example
OMP_NUM_THREADS=4 /share/Apps/examples/simple_jobs/laplace_omp &lt;&lt; EOF
400
EOF

exit
&lt;/pre&gt;


---

# Submit script for MPI Job

&lt;pre&gt;
#!/bin/bash
#SBATCH --partition=lts
#SBATCH --time=1:00:00
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=2
## For --partition=im1080, 
###  use --ntasks-per-node=20
### and --qos=nogpu
#SBATCH --job-name=myjob

source /etc/profile.d/zlmod.sh
module load mvapich2

cd ${SLURM_SUBMIT_DIR}
srun ./myjob &lt; filename.in &gt; filename.out

# Example
srun -n 4 /share/Apps/examples/simple_jobs/laplace_mpi &lt;&lt; EOF
400
EOF

exit
&lt;/pre&gt;


---

# Submit script for LAMMPS GPU job

&lt;pre&gt;
#!/bin/bash
#SBATCH --partition=hawkgpu
# Directives can be combined on one line
#SBATCH --time=1:00:00
#SBATCH --nodes=1
# 1 CPU can be be paired with only 1 GPU
# 1 GPU can be paired with all 24 CPUs
#SBATCH --ntasks-per-node=6
#SBATCH --gres=gpu:1
# Need both GPUs, use --gres=gpu:2
#SBATCH --job-name myjob

source /etc/profile.d/zlmod.sh
cd ${SLURM_SUBMIT_DIR}
# Load LAMMPS Module
module load lammps
# Run LAMMPS for input file in.lj
srun $(which lmp) -in in.lj -sf gpu -pk gpu 1 

exit
&lt;/pre&gt;


---

# Useful SLURM Directives

| SLURM Directive | Description |
|:---------------:|:-----------:|
|  --partition=queuename | Submit job to the &lt;em&gt;queuename&lt;/em&gt; partition | 
|  --time=hh:mm:ss | Request resources to run job for &lt;em&gt;hh&lt;/em&gt; hours, &lt;em&gt;mm&lt;/em&gt; minutes and &lt;em&gt;ss&lt;/em&gt; seconds |
|  --nodes=m | Request resources to run job on &lt;em&gt;m&lt;/em&gt; nodes |
|  --ntasks-per-node=n | Request resources to run job on &lt;em&gt;n&lt;/em&gt; processors on each node requested |
|  --ntasks=n | Request resources to run job on a total of &lt;em&gt;n&lt;/em&gt; processors | 
|  --job-name=jobname | Provide a name, &lt;em&gt;jobname&lt;/em&gt; to your job |
|  --output=filename.out | Write SLURM standard output to file filename.out |
|  --error=filename.err | Write SLURM standard error to file filename.err |
|  --mail-type=events | Send an email after job status events is reached. events can be NONE, BEGIN, END, FAIL, REQUEUE, ALL, TIME&amp;#95;LIMIT(&amp;#95;90,80)|
|  --mail-user=address | Address to send email |
|  --account=mypi | charge job to the __mypi__ account |
|  --gres=gpu:# | Specifies number of gpus requested in the gpu partitions, min 1 cpu per gpu | 

---

# Useful SLURM Directives (contd)

* SLURM can also take short hand notation for the directives

| Long Form | Short Form |
|:---------:|:----------:|
| --partition=queuename | -p queuename |
| --time=hh:mm:ss | -t hh:mm:ss |
| --nodes=m | -N m |
| --ntasks=n | -n n |
| --account=mypi | -A mypi |
| --job-name=jobname | -J jobname |
| --output=filename.out | -o filename.out |


---

# SLURM Filename Patterns

* SLURM allows for a filename pattern to contain one or more replacement
  symbols, which are a percent sign "%" followed by a letter (e.g. %j). 

| Pattern | Description |
|:-------:|:-----------:|
| %A |    Job array's master job allocation number |
| %a |    Job array ID (index) number |
| %J |    jobid.stepid of the running job (e.g. "128.0") |
| %j |    jobid of the running job |
| %N |    short hostname. This will create a separate IO file per node |
| %n |    Node identifier relative to current job (e.g. "0" is the first node of the running job) This will create a separate IO file per node |
| %s |    stepid of the running job |
| %t |    task identifier (rank) relative to current job. This will create a separate IO file per task |
| %u |    User name |
| %x |    Job name |



---

# Useful SLURM environmental variables

* SLURM creates environmental variables that can be used in the submit script.

| SLURM Command | Description | 
|:-------------:|:-----------:|
| SLURM_SUBMIT_DIR | Directory where job was submitted |
| SLURM_NTASKS | Total number of cores for job |
| SLURM_JOB_NUM_NODES | Total number of nodes for job |
| SLURM_JOB_NODELIST | List of nodes assigned to your job |
| SLURM_JOB_ID or SLURM_JOBID | Job ID number given to this job |
| SLURM_JOB_PARTITION | Partition job is running on |
| SLURM_JOB_NAME | Name of the job |


---
class: inverse, middle

# Using Interactive Apps on Open OnDemand
### Jupyter Lab &amp; Notebooks
### RStudio
### MATLAB
### Virtual Desktops
### and more


---

# Jupyter Lab &amp; Notebooks

* Launch Jupyter Lab &amp; Notebooks with the anaconda module.

* Use your own conda environment provided you have jupyter notebooks installed in your environment

* By default,
   - 1 node per job

* See https://go.lehigh.edu/python for list of available packages in each anaconda/python module.

* [Demo](https://confluence.cc.lehigh.edu/x/G5JVBw)

* For homeworks: Use Class Apps &gt; JupyterLab Server
   * Max wall time is 2 hours.
   * Not meant for intensive calculations as resources are oversubscribed

---

# Exercise

* Login to Open OnDemand

* Start a terminal: Clusters &gt; Sol Shell Access

* Run the following commands


```bash
cd ~/che396_s2022_proj/$USER
cp -r ../shared/python_seminar .
```

* Start a JupyterLab session: Class Apps &gt; JupyterLab Server &gt; Launch

* When the session starts, navigate to `~/che396_s2022_proj/$USER/python_seminar` and open `datavis.ipynb`

* Run through the notebook to learn about Data Visualization with Matplotlib and Seaborn

---

# Other Applications

* [RStudio](https://confluence.cc.lehigh.edu/x/NpBVBw)

* [Virtual Desktops](https://confluence.cc.lehigh.edu/x/g5BVBw)

* [MATLAB](https://confluence.cc.lehigh.edu/x/5ZBVBw)

* [VMD](https://confluence.cc.lehigh.edu/x/yJRVBw)

* [Visualization Tools](https://confluence.cc.lehigh.edu/x/qan0BQ)

---
class: inverse, middle

# Research Computing Services 


---

# HPC Seminars

* Fridays, 2:00PM - 4:00PM via Zoom

| Date | Topic |
|:-----|:------|
| Feb. 3 | Research Computing Resources at Lehigh |
| Feb. 10 | Linux: Basic Commands &amp; Environment |
| Feb. 17 | Using SLURM scheduler on Sol |
| Feb. 24 | Python Programming |
| Mar. 3 | R Programming |
| Mar. 10 | Introduction to Open OnDemand |
| Mar. 17 | Data Visualization with Python |
| Mar. 24 | Data Visualization with R |
| Mar. 31 | BYOS: Container on HPC resources |
| Apr. 7 | Object Oriented Programking with Python |
| Apr. 14 | Shiny Apps in R |

* Register at https://go.lehigh.edu/hpcseminars

---

# Workshops

* We provide full day workshops on programming topics.
- [Summer Workshops](https://go.lehigh.edu/hpcworkshops)
  - Modern Fortran Programming (Summer 2015, 2021)
  - C Programming (Summer 2015, 2021)
  - HPC Parallel Programming Workshop (Summer 2017, 2018, 2021)
  - Quantum Chemistry Workshop (Summer 2021)
* We also host full day XSEDE workshops.
  - XSEDE HPC Monthly Workshop: OpenACC (Dec. 2014).
  - XSEDE HPC Summer BootCamp: OpenMP, OpenACC, MPI and Hybrid Programming (Jun.
    2015 - 2019).
  - XSEDE HPC Monthly Workshop: Big Data (Nov. 2015, May 2017).




---

# Getting Help

* Issue with running jobs or need help to get started: 
  * Open a help ticket: &lt;http://lts.lehigh.edu/help&gt;
  * See [guidelines for faster response](https://confluence.cc.lehigh.edu/x/KJVVBw)
- Investing in Sol
  - Contact Alex Pacheco or Steve Anthony
* More Information
  * [Research Computing](https://go.lehigh.edu/hpc)
  * [Research Computing Seminars](https://go.lehigh.edu/hpcseminars)
  * [Condo Program](https://confluence.cc.lehigh.edu/x/EgL5Bg)
  * [Proposal Assistance](https://confluence.cc.lehigh.edu/x/FgL5Bg)
  * [Data Management Plans](http://libraryguides.lehigh.edu/researchdatamanagement)
* Subscribe
     * [Research Computing Mailing List](https://lists.lehigh.edu/mailman/listinfo/hpc-l)
     * [HPC Training Google Groups](mailto:hpctraining-list+subscribe@lehigh.edu)

---
class: inverse middle

# Thank You!
# Questions?


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script src="addons/imgscale.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:10",
"yolo": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
